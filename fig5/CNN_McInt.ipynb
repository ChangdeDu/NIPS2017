{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T14:24:57.531438",
     "start_time": "2017-09-08T14:24:57.503804"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Setup:\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns#nicer plots?\n",
    "sns.reset_orig()\n",
    "import scipy.stats as sps\n",
    "from scipy.stats import chi\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "fig_size = [18,18]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# Tensorflow needs to be LAST import\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T14:25:02.020681",
     "start_time": "2017-09-08T14:24:58.774204"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Loading the data from VGG Conv2 activations\n",
    "Data=np.load('/gpfs01/bethge/home/dklindt/David/publish/data.npz')\n",
    "print(Data.keys())\n",
    "Y=Data.f.responses\n",
    "Y=Y.T# N x D\n",
    "X=Data.f.images\n",
    "X=np.transpose(X,axes=[0,3,1,2])# NCHW\n",
    "loc_y=Data.f.loc_y\n",
    "loc_x=Data.f.loc_x\n",
    "feature_maps=Data.f.feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! splits data into train,val,test and adds poisson-like noise\n",
    "def splitting_data(X,Y,num_train,num_val,num_test=2**10,noise=True,mean_response=.1):\n",
    "    \n",
    "    N=Y.shape[0]#number of neurons\n",
    "    \n",
    "    X_train=X[:num_train,:,:,:]\n",
    "    X_val=X[num_train:num_train+num_val,:,:,:]\n",
    "    X_test=X[num_train+num_val:num_train+num_val+num_test,:,:,:]\n",
    "    \n",
    "    Y_train=np.zeros([N,num_train])\n",
    "    Y_val=np.zeros([N,num_val])\n",
    "    #Y_test=np.zeros([N,num_test])\n",
    "    GT_test=np.zeros([N,num_test])\n",
    "    \n",
    "    for n in range(1000):\n",
    "        tmp_mean = np.mean(Y[n,:])\n",
    "        Y_train[n,:] = Y[n,:num_train] / tmp_mean * mean_response\n",
    "        Y_val[n,:] = Y[n,num_train:num_train+num_val] / tmp_mean * mean_response\n",
    "        GT_test[n,:] = Y[n,num_train+num_val:num_train+num_val+num_test] / tmp_mean * mean_response\n",
    "    \n",
    "    #Poisson-like noise\n",
    "    if noise:\n",
    "        Y_train += np.random.normal(0,np.sqrt(np.abs(Y_train)),Y_train.shape)\n",
    "        Y_val +=  np.random.normal(0,np.sqrt(np.abs(Y_val)),Y_val.shape)\n",
    "        #Y_test =  GT_test + np.random.normal(0,np.sqrt(np.abs(GT_test)),GT_test.shape)\n",
    "    \n",
    "    #Poisson noise\n",
    "    #if noise:\n",
    "    #    Y_train += np.random.poisson(Y_train)\n",
    "    #    Y_val +=  np.random.poisson(Y_val)\n",
    "    #    Y_test =  GT_test + np.random.poisson(GT_test)\n",
    "    \n",
    "    return Y_train,Y_val,X_train,X_val,X_test,GT_test#,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T11:48:58.224509",
     "start_time": "2017-09-08T11:48:58.156395"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Spike triggered average - as initialization or else\n",
    "def STA(X,Y,crop,s2=13):\n",
    "    #In:\n",
    "    #X - stimuli, stimulus_size x number_of_data\n",
    "    #Y - responses, number_of_neurons x number_of_data\n",
    "    #smooth - size of smoothing gaussian (=s2), if not provided, no smoothing\n",
    "    #Out\n",
    "    #sta - spike triggered average, number_of_neurons x stimulus_size\n",
    "    s=np.sqrt(X.shape[0]).astype(int)\n",
    "    d=X.shape[1]\n",
    "    n=Y.shape[0]\n",
    "    \n",
    "    sta = ((X.dot(Y.T))/Y.shape[1]).T\n",
    "    sta = sta.reshape([n,s*s])\n",
    "    \n",
    "    #Smoothing\n",
    "    x = np.linspace(1, s2, s2)\n",
    "    y = np.linspace(s2, 1, s2)\n",
    "    xm, ym = np.meshgrid(x, y)\n",
    "\n",
    "    centre = [s2/2+.5, s2/2+.5]\n",
    "    ind_tmp = (np.abs(xm-centre[0]) < s2) & (np.abs(ym-centre[1]) < s2)\n",
    "    rf_tmp = np.zeros((s2,s2))\n",
    "    rf_tmp[ind_tmp] = np.sqrt( (centre[0] - xm[ind_tmp])**2 +\n",
    "                      (centre[1] - ym[ind_tmp])**2 )\n",
    "    rf_tmp[ind_tmp] = (sps.norm.pdf(rf_tmp[ind_tmp],0,s2**(1/4)))\n",
    "\n",
    "    normal=rf_tmp\n",
    "    #plt.imshow(normal)\n",
    "    #plt.show()\n",
    "    sta_smooth=np.zeros(sta.shape)\n",
    "    sta_r1=np.zeros([n,s,s])\n",
    "    for i in range(n):\n",
    "        sta_smooth[i,:] = signal.convolve2d(((sta[i,:])**2).reshape([s,s]),\n",
    "            normal,mode='same').reshape(s**2)\n",
    "        #rank one approx\n",
    "        U,S,V = np.linalg.svd(sta[i,:].reshape(s,s))\n",
    "        S1 = np.zeros(S.shape)\n",
    "        S1[0] = S[0]\n",
    "        tmp1 = U.dot(np.diag(S1).dot(V))\n",
    "        sta_r1[i,:,:] = signal.convolve2d((tmp1**2),\n",
    "            normal,mode='same')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #cropping\n",
    "    ind = np.int((s-crop)/2)\n",
    "    sta = sta.reshape([n,s,s])\n",
    "    sta = sta[:,ind:s-ind,ind:s-ind]\n",
    "    sta = sta.reshape([n,crop**2])\n",
    "    sta_s = sta_smooth.reshape([n,s,s])\n",
    "    sta_s = sta_s[:,ind:s-ind,ind:s-ind]\n",
    "    sta_s = sta_s.reshape([n,crop**2])\n",
    "    sta_r1 = sta_r1[:,ind:s-ind,ind:s-ind]\n",
    "    sta_r1 = sta_r1.reshape([n,crop**2])\n",
    "        \n",
    "    return sta, sta_s#, sta_r1\n",
    "\n",
    "\n",
    "#######rank one approx\n",
    "'''\n",
    "for i in range(3):\n",
    "    tmp = tmp_sta[356+i,:].reshape(32,32).copy()\n",
    "    plt.imshow(tmp)\n",
    "    plt.show()\n",
    "    u,s,v = np.linalg.svd(tmp)\n",
    "    print(u.shape,s.shape,v.shape)\n",
    "    s1 = np.zeros(s.shape)\n",
    "    s1[0] = s[0]\n",
    "    tmp1 = u.dot(np.diag(s1).dot(v))\n",
    "    plt.imshow(tmp1)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T14:25:07.528314",
     "start_time": "2017-09-08T14:25:06.892315"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Model tensorflow\n",
    "from tensorflow.contrib import layers\n",
    "class ModelGraph:\n",
    "    def __init__(self, s, rT, rA, init_scaleK, init_scaleT, N, num_kern,\n",
    "                 init_mask=np.array([]),\n",
    "                 init_weights=np.array([]), init_kernel=np.array([])):\n",
    "        #Inputs:\n",
    "        #        s*s - size of the image\n",
    "        #        s2*s2 - size of the kernel\n",
    "        #        rM/W - regularization weight Mask / Weights\n",
    "        #        N - number of neurons\n",
    "        #        num_kern - number of kernels per conv layer\n",
    "        \n",
    "        self.graph = tf.Graph()#new tf graph\n",
    "        with self.graph.as_default():#use it as default\n",
    "            \n",
    "            tf_seed=3973\n",
    "            #input tensor of shape NCHW!\n",
    "            self.X = tf.placeholder(tf.float32,shape=[None,3,s[0],s[0]])\n",
    "            #output: N x None\n",
    "            self.Y = tf.placeholder(tf.float32)             \n",
    "            \n",
    "            #WK Kernel - filter / tensor of shape H-W-InChannels-OutChannels\n",
    "            \n",
    "            #batch normalization settings\n",
    "            self.istrain = tf.placeholder(tf.bool)\n",
    "            \n",
    "            bn_params = dict(center=True,\n",
    "                             scale=False,\n",
    "                             is_training=self.istrain,\n",
    "                             variables_collections=['batch_norm_ema'])\n",
    "            \n",
    "            #Layer: Conv1\n",
    "            self.conv1 = layers.convolution2d(\n",
    "                inputs=self.X,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[0],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv1')\n",
    "            with tf.variable_scope('conv1', reuse=True):\n",
    "                self.WK1 = tf.get_variable('weights')\n",
    "                \n",
    "            #Layer: Conv2\n",
    "            self.conv2 = layers.convolution2d(\n",
    "                inputs=self.conv1,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[1],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv2')\n",
    "            with tf.variable_scope('conv2', reuse=True):\n",
    "                self.WK2 = tf.get_variable('weights')\n",
    "                \n",
    "            #Layer: Conv3\n",
    "            self.conv3 = layers.convolution2d(\n",
    "                inputs=self.conv2,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[2],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv3')\n",
    "            with tf.variable_scope('conv3', reuse=True):\n",
    "                self.WK3 = tf.get_variable('weights')\n",
    "            \n",
    "            #batch_norm op\n",
    "            self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            \n",
    "            #WT read out tensor\n",
    "            self.WT_init = tf.random_normal([num_kern[-1],s[2],s[2],N],init_scaleT[0],init_scaleT[1])\n",
    "            self.WT = tf.Variable(self.WT_init,dtype=tf.float32,name='WT')\n",
    "            \n",
    "            #Predicted Output\n",
    "            self.Y_ = tf.transpose(tf.einsum('dchw,chwn->dn',self.conv3,self.WT))#NxD\n",
    "            \n",
    "            #Regularization\n",
    "            self.regT = tf.reduce_sum(tf.square(self.WT)) #L2 Loss on read-out tensor\n",
    "            self.regA = tf.reduce_mean(tf.reduce_sum(tf.abs(self.Y_),0)) #mean L1 Loss on Activations\n",
    "            \n",
    "            #Define a loss function\n",
    "            self.res = self.Y_-self.Y\n",
    "            self.MSE = tf.reduce_sum(tf.reduce_mean(self.res * self.res,1))\n",
    "            self.loss = self.MSE + rT*self.regT + rA*self.regA\n",
    "            \n",
    "            #Define a training graph\n",
    "            self.step_size= tf.placeholder(tf.float32)\n",
    "            self.training = tf.train.AdamOptimizer(self.step_size).minimize(self.loss)\n",
    "            \n",
    "            # Create a saver.\n",
    "            self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T14:25:08.083601",
     "start_time": "2017-09-08T14:25:07.530244"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Training CNN-NL\n",
    "def train(init_scaleK,init_scaleT,init_lr,num_kern,max_runs,rT,rA,s,N,\n",
    "          X_train,X_val,X_test,Y_train,Y_val,GT_test,batch_size,verbose=False):\n",
    "    \n",
    "    #Storing:\n",
    "    MSE_train = [] # MSE on train set, reps x runs/100\n",
    "    tmp_MT = []#dummy for storing the above during repetition for Xruns\n",
    "    MSE_val = []#MSE on validation set, reps x runs/100\n",
    "    tmp_MV = []#dummy for storing the above during repetition for Xruns\n",
    "    MSE_test = []#MSE on test set, reps x 1\n",
    "    WK1 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK1 = []#dummy during run\n",
    "    WK2 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK2 = []#dummy during run\n",
    "    WK3 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK3 = []#dummy during run\n",
    "    WT = []# Read Out Weights - store best weights, reps x s x s\n",
    "    tmp_WT = []#dummy during run\n",
    "    FEV = []#fraction of explained variance, reps x 1\n",
    "\n",
    "    #calculate test variance\n",
    "    gt_test_var = np.sum(np.var(GT_test,axis =1))#explainable output variance\n",
    "    \n",
    "    #initialize current attributes\n",
    "    lr=init_lr\n",
    "    # flags for early stopping\n",
    "    stop_flag = 0\n",
    "    sstop=0\n",
    "\n",
    "    #Init model class\n",
    "    model = ModelGraph(s,rT,rA,init_scaleK,init_scaleT,N,num_kern)\n",
    "\n",
    "    #validation feed can be outside loop:\n",
    "    #feed_val = {model.X:X_val,\n",
    "    #            model.Y:Y_val,model.istrain:False}\n",
    "    #feed_test = {model.X:X_test,model.Y:GT_test,model.istrain:False}\n",
    "    \n",
    "    #Split up because too large for GPU memory...\n",
    "    num_test=np.int(GT_test.shape[1]/batch_size)\n",
    "    feed_test=[]\n",
    "    for i in range(num_test):\n",
    "        feed_test.append( {model.X:X_test[i*batch_size:(i+1)*batch_size,:,:,:],\n",
    "                        model.Y:GT_test[:,i*batch_size:(i+1)*batch_size],model.istrain:False})\n",
    "    num_val=np.int(Y_val.shape[1]/batch_size)\n",
    "    feed_val=[]\n",
    "    for i in range(num_val):\n",
    "        feed_val.append( {model.X:X_val[i*batch_size:(i+1)*batch_size,:,:,:],\n",
    "                        model.Y:Y_val[:,i*batch_size:(i+1)*batch_size],model.istrain:False})\n",
    "    \n",
    "    ##Start a tf session\n",
    "    with model.graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            tf_seed=3973\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            #plot initial weights\n",
    "            if verbose:\n",
    "                print('Before Training:')\n",
    "                fig, ax = plt.subplots(1, 5, figsize=[18, 3])\n",
    "                tmp_wt=model.WT.eval()[...,0]#first neuron as example\n",
    "                for i in range(4):\n",
    "                    ax[i].imshow(tmp_wt[i,:,:].reshape(s[2],s[2]),cmap='bwr',\n",
    "                                 vmin=-np.max(abs(tmp_wt[i,:,:])),\n",
    "                                 vmax=np.max(abs(tmp_wt[i,:,:])))\n",
    "                test=np.zeros(num_test*batch_size)\n",
    "                act_loss=np.zeros(num_test)\n",
    "                for i in range(num_test):\n",
    "                    [tmp_y,\n",
    "                     act_loss[i]] = sess.run([model.Y_,model.regA],feed_test[i])\n",
    "                    test[i*batch_size:(i+1)*batch_size] = tmp_y[0]\n",
    "                ax[4].plot(GT_test[0], test, '.')\n",
    "                xx = [-.4, .4]\n",
    "                ax[4].plot(xx, xx)\n",
    "                ax[4].axis('equal')\n",
    "                ax[4].set_title('true vs predicted')\n",
    "                plt.show()\n",
    "                print('Loss: %s tensor'%(model.regT.eval()*rT),\n",
    "                      'Loss: %s activation'%(np.mean(act_loss)*rA))\n",
    "\n",
    "            #Batches - define list of starting-indices for individual batches in data set:\n",
    "            #if there is less training data than batch size\n",
    "            batch_size = np.min([batch_size,X_train.shape[0]])\n",
    "            batch_ind = np.arange(0,X_train.shape[0],batch_size)\n",
    "            #number of selected batch\n",
    "            batch = 0\n",
    "\n",
    "            for j in range(1,max_runs):\n",
    "\n",
    "                #when there is no further complete batch\n",
    "                if batch==len(batch_ind):\n",
    "                    #shuffle data and start again:\n",
    "                    ind = np.random.permutation(X_train.shape[0])\n",
    "                    X_train = X_train[ind,:,:,:]\n",
    "                    Y_train = Y_train[:,ind]\n",
    "                    batch = 0\n",
    "\n",
    "                #take a batch\n",
    "                X_batch = X_train[batch_ind[batch]:batch_ind[batch]+batch_size,:,:,:]\n",
    "                Y_batch = Y_train[:,batch_ind[batch]:batch_ind[batch]+batch_size]\n",
    "                batch +=1\n",
    "                \n",
    "                #Training feed:\n",
    "                feed_dict ={model.step_size:lr,model.X:X_batch,\n",
    "                            model.Y:Y_batch,model.istrain:True}\n",
    "\n",
    "                # Training with current batch:\n",
    "                sess.run([model.training, model.update_ops],feed_dict)\n",
    "                \n",
    "                #Early Stopping - check if MSE doesn't increase\n",
    "                if j%100==0:\n",
    "\n",
    "                    model.saver.save(sess, 'bn_checkpoint', global_step=int(j/100))\n",
    "\n",
    "                    # Store MSE on train:\n",
    "                    \n",
    "                    tmp_MT.append(model.MSE.eval(feed_dict))\n",
    "\n",
    "                    #check MSE on validation set and store the parameters\n",
    "                    #tmp_MV.append(model.MSE.eval(feed_val))\n",
    "                    val=np.zeros(num_val)\n",
    "                    for i in range(num_val):\n",
    "                        val[i]=sess.run(model.MSE,feed_val[i])\n",
    "                    tmp_MV.append(np.mean(val))\n",
    "                    tmp_WK1.append(model.WK1.eval())\n",
    "                    tmp_WK2.append(model.WK2.eval())\n",
    "                    tmp_WK3.append(model.WK3.eval())\n",
    "                    tmp_WT.append(model.WT.eval())\n",
    "\n",
    "                    #Best run\n",
    "                    if len(tmp_MV)>5:#burn in \n",
    "                        tmp_min_ind = np.argmin(tmp_MV[5:])+5\n",
    "                    else:\n",
    "                        tmp_min_ind = len(tmp_MV)-1\n",
    "\n",
    "                    ##Analytics - Display progress\n",
    "                    if verbose:\n",
    "                        #to calculate FEV\n",
    "                        test=np.zeros(num_test)\n",
    "                        for i in range(num_test):\n",
    "                            test[i]=sess.run(model.MSE,feed_test[i])\n",
    "                        MSE_gt = np.mean(test)\n",
    "                        print('Runs: %s; MSE - train: %s, val: %s; lr = %s'%\n",
    "                              (j,tmp_MT[-1],tmp_MV[-1],lr))\n",
    "                        print('latest: ',tmp_MV[-1])\n",
    "                        print('FEV = ',(1 - (MSE_gt/gt_test_var)))\n",
    "                        print('best: ',tmp_min_ind,tmp_MV[tmp_min_ind])\n",
    "                        \n",
    "                        fig, ax = plt.subplots(1, 6, figsize=[18, 3])\n",
    "                        tmp_wt=model.WT.eval()[...,0]#first neuron as example\n",
    "                        for i in range(4):\n",
    "                            ax[i].imshow(tmp_wt[i,:,:].reshape(s[2],s[2]),cmap='bwr',\n",
    "                                         vmin=-np.max(abs(tmp_wt[i,:,:])),\n",
    "                                         vmax=np.max(abs(tmp_wt[i,:,:])))\n",
    "                        test=np.zeros(num_test*batch_size)\n",
    "                        act_loss=np.zeros(num_test)\n",
    "                        for i in range(num_test):\n",
    "                            [tmp_y,\n",
    "                             act_loss[i]] = sess.run([model.Y_,model.regA],feed_test[i])\n",
    "                            test[i*batch_size:(i+1)*batch_size] = tmp_y[0]\n",
    "                        ax[4].plot(GT_test[0], test, '.')\n",
    "                        xx = [-.4, .4]\n",
    "                        ax[4].plot(xx, xx)\n",
    "                        ax[4].axis('equal')\n",
    "                        ax[4].set_title('true vs predicted')\n",
    "                        ax[5].plot(tmp_MV)\n",
    "                        ax[5].plot(tmp_MT)\n",
    "                        ax[5].set_ylim([min(tmp_MT),2*np.median(tmp_MV)-min(tmp_MT)])\n",
    "                        ax[5].legend(['MSE Val','MSE Train'])\n",
    "                        plt.show()\n",
    "                        print('Loss: %s tensor'%(model.regT.eval()*rT),\n",
    "                              'Loss: %s activation'%(np.mean(act_loss)*rA))\n",
    "                        \n",
    "\n",
    "                    ##Early Stopping - if latest validation MSE is not minimum\n",
    "                    if tmp_min_ind != len(tmp_MV)-1:\n",
    "                        stop_flag +=1\n",
    "                        if stop_flag>=8:\n",
    "                            lr *= .1\n",
    "                            #set back to previous best?\n",
    "                            #model.saver.restore(sess, 'bn_checkpoint-%s'%(tmp_min_ind+1))\n",
    "                            #print('back to MSE-val = ',model.MSE_test.eval(feed_val))\n",
    "                            stop_flag = 0\n",
    "                            sstop +=1\n",
    "                            if sstop==3:#lower the lr x times\n",
    "                                break\n",
    "                                \n",
    "                    else:#if latest value is best, reset\n",
    "                        stop_flag = 0\n",
    "\n",
    "            #Best run\n",
    "            tmp_min_ind = np.argmin(tmp_MV)\n",
    "\n",
    "            #Store MSEs\n",
    "            MSE_train = tmp_MT#list\n",
    "            MSE_val = tmp_MV#list\n",
    "            \n",
    "            #Store best weights (i.e. lowest validation MSE)\n",
    "            WK1 = tmp_WK1[tmp_min_ind]#s2 x s2\n",
    "            WK2 = tmp_WK2[tmp_min_ind]#s2 x s2\n",
    "            WK3 = tmp_WK3[tmp_min_ind]#s2 x s2\n",
    "            WT = tmp_WT[tmp_min_ind]#\n",
    "\n",
    "            #Assign the best weights to model graph \n",
    "            model.saver.restore(sess, './bn_checkpoint-%s'%(tmp_min_ind+1))\n",
    "            \n",
    "            #clean checkpoints\n",
    "            files = os.listdir()\n",
    "            for file in files:\n",
    "                if file.startswith(\"bn_checkpoint\"):\n",
    "                    os.remove(file)\n",
    "                    \n",
    "            # Test MSE prediction\n",
    "            #MSE_test = sess.run(model.MSE,feed_test)#1 x 1\n",
    "            test=np.zeros(num_test)\n",
    "            #test_L=np.zeros(num_test)\n",
    "            for i in range(num_test):\n",
    "                test[i]=sess.run(model.MSE,feed_test[i])\n",
    "            #    test_L[i]=sess.run(model.poisson,feed_test[i])\n",
    "            MSE_test = np.mean(test)\n",
    "            #Loss_test = np.mean(Loss)\n",
    "            \n",
    "            #FEV - fraction of explainable variance\n",
    "            FEV = 1 - (MSE_test/gt_test_var)#1 x 1\n",
    "\n",
    "            #Predicted outputs\n",
    "            #Y_ = sess.run(model.Y_,feed_test)\n",
    "            test=np.zeros([N,num_test*batch_size])\n",
    "            for i in range(num_test):\n",
    "                test[:,i*batch_size:(i+1)*batch_size]=sess.run(model.Y_,feed_test[i])\n",
    "            Y_ = test\n",
    "            \n",
    "            #FEV per cell\n",
    "            fev_cell=1-(np.mean((Y_-GT_test)**2,1)/np.var(GT_test,1))\n",
    "\n",
    "            #Output\n",
    "            log=('Stop at run %s; MSE on validation set: %s'% (j,MSE_val[tmp_min_ind]),\n",
    "                  'MSE on test set: %s; FEV: %s' % (MSE_test, FEV))\n",
    "            print(log)\n",
    "\n",
    "    return (WK1,WK2,WK3,WT,MSE_train,MSE_val,MSE_test,FEV,fev_cell,Y_,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Cross validation for parameters...\n",
    "np_seed=3567\n",
    "np.random.seed(np_seed)\n",
    "start=time.time()\n",
    "#Parameters:\n",
    "N=1000\n",
    "s1=44#width=heigth of image\n",
    "s2=5\n",
    "s3=32\n",
    "n_B = [32,64,4]#number of bases to learn\n",
    "init_scaleK = .01#Kernel\n",
    "init_scaleT = [0,.01]#Read out Weights\n",
    "lr= .001 # initial learning rate\n",
    "rT = [.0001,.001,.01,.1,1]#regularization on Mask\n",
    "rA = [.0001,.001,.01,.1,1]\n",
    "#loop parameters\n",
    "max_runs = 15000 # training steps\n",
    "batch_size = 64\n",
    "    \n",
    "#Clean previous checkpoints\n",
    "files = os.listdir()\n",
    "for file in files:\n",
    "    if file.startswith(\"bn_checkpoint\"):\n",
    "        os.remove(file)\n",
    "\n",
    "#prepare data\n",
    "num_test=batch_size*92\n",
    "num_train=2**15\n",
    "num_val=np.int(num_train/8)\n",
    "num_train-=num_val\n",
    "\n",
    "#Y_train_orig,X_train_orig,X_test,GT_test = generate_data(X,Y,num_test)\n",
    "GT_test = GT_test_orig[:,:num_test]\n",
    "X_test = X_test_orig[:num_test,:,:,:]\n",
    "X_train=X_train_orig[:num_train,:,:,:]\n",
    "X_val=X_train_orig[num_train:num_train+num_val,:,:,:]\n",
    "Y_train=Y_train_orig[:,:num_train]\n",
    "Y_val=Y_train_orig[:,num_train:num_train+num_val]\n",
    "\n",
    "X_sta=np.mean(X_train,axis=1).reshape([-1,s1**2]).T\n",
    "tmp_sta,_ = STA(X_sta,Y_train,s3)\n",
    "\n",
    "FEV=np.zeros([5,5])\n",
    "for t in range(5):\n",
    "    for a in range(5):\n",
    "        start=time.time()\n",
    "        WK1,WK2,WK3,WT_tmp,MSE_train,MSE_val,MSE_test,FEV[a,t],fev_c,Y_tmp,log_tmp = train(init_scaleK,\n",
    "                                                           init_scaleT,lr,n_B,\n",
    "                                                           max_runs,rT[t],rA[a],[s1,s2,s3],N,X_train,X_val,X_test,\n",
    "                                                           Y_train,Y_val,GT_test,batch_size,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-08T12:41:13.138Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##over different population sizes and tensor regs, act reg = .1\n",
    "reps = 3\n",
    "np_seed=3567\n",
    "np.random.seed(np_seed)\n",
    "start=time.time()\n",
    "#Parameters:\n",
    "N=1000\n",
    "s1=44#width=heigth of image\n",
    "s2=5\n",
    "s3=32\n",
    "n_B = [32,64,4]#number of bases to learn\n",
    "init_scaleK = .01#Kernel\n",
    "init_scaleT = [0,.01]#Read out Weights\n",
    "lr= .001 # initial learning rate\n",
    "\n",
    "#best regularization (ranges) found via exploration\n",
    "rT = np.geomspace(.05,5,5)#regularization on Mask\n",
    "rA = .1\n",
    "#loop parameters\n",
    "max_runs = 15000 # training steps\n",
    "batch_size = 64\n",
    "    \n",
    "#Clean previous checkpoints\n",
    "files = os.listdir()\n",
    "for file in files:\n",
    "    if file.startswith(\"bn_checkpoint\"):\n",
    "        os.remove(file)\n",
    "\n",
    "data_sets=2**np.arange(9,17)\n",
    "\n",
    "#Data, keep test set for later\n",
    "num_test = 2**10\n",
    "ind = np.random.choice(X.shape[0],X.shape[0],replace=False)\n",
    "X_TEST = X[ind[:num_test],...]\n",
    "GT_TEST = (Y[:,ind[:num_test]].T / np.mean(Y,1)).T * .1#mean response=.1\n",
    "X_try = X[ind[num_test:],...]\n",
    "Y_try = Y[:,ind[num_test:]]\n",
    "\n",
    "\n",
    "FEV=np.zeros([reps,len(data_sets),5])\n",
    "Val=np.zeros([reps,len(data_sets),5])\n",
    "\n",
    "for rep in range(reps):\n",
    "    print('repetition ',rep)\n",
    "    for d in range(len(data_sets)):\n",
    "        num_train = np.min([data_sets[d],Y_try.shape[1]])\n",
    "        print('data=',num_train)\n",
    "\n",
    "        num_val = np.min([num_train//8,num_test])#number of val\n",
    "        num_train -= num_val#number of train\n",
    "        Y_train,Y_val,X_train,X_val,_,_ = splitting_data(X=X_try,Y=Y_try,\n",
    "           num_train=num_train,num_test=0,num_val=num_val,noise=True,mean_response=.1)\n",
    "\n",
    "        print('Images for Training: %s, Validation: %s, Testing %s'%(\n",
    "            Y_train.shape[1],Y_val.shape[1],GT_TEST.shape[1]))\n",
    "\n",
    "\n",
    "        X_sta=np.mean(X_train,axis=1).reshape([-1,s1**2]).T\n",
    "        tmp_sta,_ = STA(X_sta,Y_train,s3)\n",
    "        \n",
    "        for r in range(5):\n",
    "            print('reg = ',rT[r])\n",
    "            WK1,WK2,WK3,WT_tmp,MSE_train,MSE_val,MSE_test,FEV[rep,d,r],fev_c,Y_tmp,log_tmp = train(init_scaleK,\n",
    "                                                               init_scaleT,lr,n_B,\n",
    "                                                               max_runs,rT[r],rA,[s1,s2,s3],N,X_train,X_val,X_TEST,\n",
    "                                                               Y_train,Y_val,GT_TEST,batch_size,verbose=False)\n",
    "            Val[rep,d,r] = np.min(MSE_val)\n",
    "            \n",
    "np.savez('McIntosh_fit',FEV=FEV,Val=Val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
