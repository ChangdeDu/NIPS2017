{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "#! Setup:\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns#nicer plots?\n",
    "sns.reset_orig()\n",
    "import scipy.stats as sps\n",
    "from scipy.stats import chi\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "fig_size = [18,18]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# Tensorflow needs to be LAST import\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T11:19:37.668473",
     "start_time": "2017-09-08T11:19:34.019949"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loc_x', 'feature_maps', 'loc_y', 'responses', 'images']\n"
     ]
    }
   ],
   "source": [
    "#! Loading the data from VGG Conv2 activations\n",
    "\n",
    "Data=np.load('/gpfs01/bethge/home/dklindt/David/publish/fig5/more_types/data_all.npz')\n",
    "\n",
    "print(Data.keys())\n",
    "Y=Data.f.responses\n",
    "Y=Y.T# N x D\n",
    "X=Data.f.images\n",
    "X=np.transpose(X,axes=[0,3,1,2])# NCHW\n",
    "loc_y=Data.f.loc_y\n",
    "loc_x=Data.f.loc_x\n",
    "feature_maps=Data.f.feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:57:43.962771",
     "start_time": "2017-09-08T10:57:43.917135"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! splits data into train,val,test and adds poisson-like noise\n",
    "def splitting_data(X,Y,num_train,num_val,num_test=2**10,noise=True,mean_response=.1):\n",
    "    \n",
    "    N=Y.shape[0]#number of neurons\n",
    "    \n",
    "    X_train=X[:num_train,:,:,:]\n",
    "    X_val=X[num_train:num_train+num_val,:,:,:]\n",
    "    X_test=X[num_train+num_val:num_train+num_val+num_test,:,:,:]\n",
    "    \n",
    "    Y_train=np.zeros([N,num_train])\n",
    "    Y_val=np.zeros([N,num_val])\n",
    "    #Y_test=np.zeros([N,num_test])\n",
    "    GT_test=np.zeros([N,num_test])\n",
    "    \n",
    "    for n in range(1000):\n",
    "        tmp_mean = np.mean(Y[n,:])\n",
    "        Y_train[n,:] = Y[n,:num_train] / tmp_mean * mean_response\n",
    "        Y_val[n,:] = Y[n,num_train:num_train+num_val] / tmp_mean * mean_response\n",
    "        GT_test[n,:] = Y[n,num_train+num_val:num_train+num_val+num_test] / tmp_mean * mean_response\n",
    "    \n",
    "    #Poisson-like noise\n",
    "    if noise:\n",
    "        Y_train += np.random.normal(0,np.sqrt(np.abs(Y_train)),Y_train.shape)\n",
    "        Y_val +=  np.random.normal(0,np.sqrt(np.abs(Y_val)),Y_val.shape)\n",
    "        #Y_test =  GT_test + np.random.normal(0,np.sqrt(np.abs(GT_test)),GT_test.shape)\n",
    "    \n",
    "    #Poisson noise\n",
    "    #if noise:\n",
    "    #    Y_train += np.random.poisson(Y_train)\n",
    "    #    Y_val +=  np.random.poisson(Y_val)\n",
    "    #    Y_test =  GT_test + np.random.poisson(GT_test)\n",
    "    \n",
    "    return Y_train,Y_val,X_train,X_val,X_test,GT_test#,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:58:00.258858",
     "start_time": "2017-09-08T10:58:00.165292"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(3):\\n    tmp = tmp_sta[356+i,:].reshape(32,32).copy()\\n    plt.imshow(tmp)\\n    plt.show()\\n    u,s,v = np.linalg.svd(tmp)\\n    print(u.shape,s.shape,v.shape)\\n    s1 = np.zeros(s.shape)\\n    s1[0] = s[0]\\n    tmp1 = u.dot(np.diag(s1).dot(v))\\n    plt.imshow(tmp1)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Spike triggered average - as initialization or else\n",
    "def STA(X,Y,crop,s2=13):\n",
    "    #In:\n",
    "    #X - stimuli, stimulus_size x number_of_data\n",
    "    #Y - responses, number_of_neurons x number_of_data\n",
    "    #smooth - size of smoothing gaussian (=s2), if not provided, no smoothing\n",
    "    #Out\n",
    "    #sta - spike triggered average, number_of_neurons x stimulus_size\n",
    "    s=np.sqrt(X.shape[0]).astype(int)\n",
    "    d=X.shape[1]\n",
    "    n=Y.shape[0]\n",
    "    \n",
    "    sta = ((X.dot(Y.T))/Y.shape[1]).T\n",
    "    sta = sta.reshape([n,s*s])\n",
    "    \n",
    "    #Smoothing\n",
    "    x = np.linspace(1, s2, s2)\n",
    "    y = np.linspace(s2, 1, s2)\n",
    "    xm, ym = np.meshgrid(x, y)\n",
    "\n",
    "    centre = [s2/2+.5, s2/2+.5]\n",
    "    ind_tmp = (np.abs(xm-centre[0]) < s2) & (np.abs(ym-centre[1]) < s2)\n",
    "    rf_tmp = np.zeros((s2,s2))\n",
    "    rf_tmp[ind_tmp] = np.sqrt( (centre[0] - xm[ind_tmp])**2 +\n",
    "                      (centre[1] - ym[ind_tmp])**2 )\n",
    "    rf_tmp[ind_tmp] = (sps.norm.pdf(rf_tmp[ind_tmp],0,s2**(1/4)))\n",
    "\n",
    "    normal=rf_tmp\n",
    "    #plt.imshow(normal)\n",
    "    #plt.show()\n",
    "    sta_smooth=np.zeros(sta.shape)\n",
    "    sta_r1=np.zeros([n,s,s])\n",
    "    for i in range(n):\n",
    "        sta_smooth[i,:] = signal.convolve2d(((sta[i,:])**2).reshape([s,s]),\n",
    "            normal,mode='same').reshape(s**2)\n",
    "        #rank one approx\n",
    "        U,S,V = np.linalg.svd(sta[i,:].reshape(s,s))\n",
    "        S1 = np.zeros(S.shape)\n",
    "        S1[0] = S[0]\n",
    "        tmp1 = U.dot(np.diag(S1).dot(V))\n",
    "        sta_r1[i,:,:] = signal.convolve2d((tmp1**2),\n",
    "            normal,mode='same')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #cropping\n",
    "    ind = np.int((s-crop)/2)\n",
    "    sta = sta.reshape([n,s,s])\n",
    "    sta = sta[:,ind:s-ind,ind:s-ind]\n",
    "    sta = sta.reshape([n,crop**2])\n",
    "    sta_s = sta_smooth.reshape([n,s,s])\n",
    "    sta_s = sta_s[:,ind:s-ind,ind:s-ind]\n",
    "    sta_s = sta_s.reshape([n,crop**2])\n",
    "    sta_r1 = sta_r1[:,ind:s-ind,ind:s-ind]\n",
    "    sta_r1 = sta_r1.reshape([n,crop**2])\n",
    "        \n",
    "    return sta, sta_s#, sta_r1\n",
    "\n",
    "\n",
    "#######rank one approx\n",
    "'''\n",
    "for i in range(3):\n",
    "    tmp = tmp_sta[356+i,:].reshape(32,32).copy()\n",
    "    plt.imshow(tmp)\n",
    "    plt.show()\n",
    "    u,s,v = np.linalg.svd(tmp)\n",
    "    print(u.shape,s.shape,v.shape)\n",
    "    s1 = np.zeros(s.shape)\n",
    "    s1[0] = s[0]\n",
    "    tmp1 = u.dot(np.diag(s1).dot(v))\n",
    "    plt.imshow(tmp1)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:58:04.572442",
     "start_time": "2017-09-08T10:58:03.900034"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Model tensorflow - mask: trainable:False\n",
    "from tensorflow.contrib import layers\n",
    "class ModelGraph:\n",
    "    def __init__(self, s, rM, rW, init_scaleK, init_scaleM,init_scaleW, N, num_kern,\n",
    "                 init_mask=np.array([]),\n",
    "                 init_weights=np.array([]), init_kernel=np.array([])):\n",
    "        #Inputs:\n",
    "        #        s*s - size of the image\n",
    "        #        s2*s2 - size of the kernel\n",
    "        #        rM/W - regularization weight Mask / Weights\n",
    "        #        N - number of neurons\n",
    "        #        num_kern - number of kernels per conv layer\n",
    "        \n",
    "        self.graph = tf.Graph()#new tf graph\n",
    "        with self.graph.as_default():#use it as default\n",
    "        \n",
    "            #input tensor of shape NCHW!\n",
    "            self.X = tf.placeholder(tf.float32,shape=[None,3,s[0],s[0]])\n",
    "            #output: N x None\n",
    "            self.Y = tf.placeholder(tf.float32)             \n",
    "            \n",
    "            #WK Kernel - filter / tensor of shape H-W-InChannels-OutChannels\n",
    "                \n",
    "            #WW - Read Out Weights\n",
    "            if init_weights.size:\n",
    "                self.WW_init = init_weights\n",
    "            else:\n",
    "                self.WW_init = tf.random_normal([num_kern[-1],N],\n",
    "                                                init_scaleW[0],init_scaleW[1])\n",
    "            \n",
    "            #WM - Mask\n",
    "            if init_mask.size:\n",
    "                self.WM_init = init_mask\n",
    "                #self.WM_initx = init_maskx\n",
    "                #self.WM_inity = init_masky\n",
    "            else:\n",
    "                self.WM_init = tf.random_normal([s[2]**2,N],0,init_scaleM)\n",
    "                #self.WM_initx = tf.random_normal([N,s[2]],0,init_scaleM)\n",
    "                #self.WM_inity = tf.random_normal([N,s[2]],0,init_scaleM)\n",
    "        \n",
    "            #batch normalization settings\n",
    "            self.istrain = tf.placeholder(tf.bool)\n",
    "            \n",
    "            bn_params = dict(center=True,\n",
    "                             scale=False,\n",
    "                             is_training=self.istrain,\n",
    "                             variables_collections=['batch_norm_ema'])\n",
    "            \n",
    "            #Layer: Conv1\n",
    "            self.conv1 = layers.convolution2d(\n",
    "                inputs=self.X,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[0],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv1')\n",
    "            with tf.variable_scope('conv1', reuse=True):\n",
    "                self.WK1 = tf.get_variable('weights')\n",
    "                \n",
    "            #Layer: Conv2\n",
    "            self.conv2 = layers.convolution2d(\n",
    "                inputs=self.conv1,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[1],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv2')\n",
    "            with tf.variable_scope('conv2', reuse=True):\n",
    "                self.WK2 = tf.get_variable('weights')\n",
    "                \n",
    "            #Layer: Conv3\n",
    "            self.conv3 = layers.convolution2d(\n",
    "                inputs=self.conv2,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[2],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv3')\n",
    "            with tf.variable_scope('conv3', reuse=True):\n",
    "                self.WK3 = tf.get_variable('weights')\n",
    "            \n",
    "            #batch_norm op\n",
    "            self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            \n",
    "            #Location layer - each neuron has one location mask\n",
    "            self.WM = tf.Variable(self.WM_init,dtype=tf.float32, name='WM',trainable=False)\n",
    "            #self.WMx = tf.Variable(self.WM_initx,dtype=tf.float32, name='WMx')\n",
    "            #self.WMy = tf.Variable(self.WM_inity,dtype=tf.float32, name='WMy')\n",
    "            #self.WMa = tf.abs(self.WM)\n",
    "            #self.WM = tf.transpose(tf.reshape(tf.einsum('ki,kj->kij', self.WMx, self.WMy),[N,s[2]**2]))\n",
    "            self.mask = tf.reshape(tf.matmul(tf.reshape(self.conv3,[-1,s[2]**2]),\n",
    "                                   self.WM),[-1,num_kern[-1],N])\n",
    "            \n",
    "            #Weighing Layer\n",
    "            self.WW = tf.Variable(self.WW_init,dtype=tf.float32, name='WW')\n",
    "            #self.WWn = self.WW# / tf.sqrt(tf.reduce_sum(tf.square(self.WW),0,keep_dims=True))\n",
    "            self.WWn = tf.abs(self.WW)\n",
    "            \n",
    "            #Predicted Output\n",
    "            self.Y_ = tf.squeeze(tf.transpose(tf.reduce_sum(tf.multiply(self.mask,\n",
    "                            self.WWn), 1, keep_dims=True)))#N x D\n",
    "            \n",
    "            #Regularization\n",
    "            self.regM = tf.reduce_sum(tf.abs(self.WM)) #L1 Loss on mask\n",
    "            self.regW = tf.reduce_sum(tf.abs(self.WW)) #L1 Loss on read out weights\n",
    "            \n",
    "            #Define a loss function\n",
    "            self.res = self.Y_-self.Y\n",
    "            self.MSE = tf.reduce_sum(tf.reduce_mean(self.res * self.res,1))\n",
    "            #self.poisson = tf.reduce_sum(tf.nn.log_poisson_loss(tf.log(self.Y_),self.Y))\n",
    "            self.loss = self.MSE + rM*self.regM + rW*self.regW\n",
    "            \n",
    "            #Define a training graph\n",
    "            self.step_size= tf.placeholder(tf.float32)\n",
    "            self.training = tf.train.AdamOptimizer(self.step_size).minimize(self.loss)\n",
    "            #self.training_noM = tf.train.AdamOptimizer(self.step_size).minimize(self.loss,\n",
    "            #                                   var_list=[self.WK,self.WW])\n",
    "            \n",
    "            # Create a saver.\n",
    "            self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:58:08.487184",
     "start_time": "2017-09-08T10:58:07.492395"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Training CNN-NL\n",
    "def train(init_scaleK,init_scaleM,init_scaleW,init_lr,num_kern,tmp_sta,max_runs,rM,rW,s,N,\n",
    "          X_train,X_val,X_test,Y_train,Y_val,GT_test,batch_size,verbose=False):\n",
    "    \n",
    "    #Storing:\n",
    "    MSE_train = [] # MSE on train set, reps x runs/100\n",
    "    tmp_MT = []#dummy for storing the above during repetition for Xruns\n",
    "    MSE_val = []#MSE on validation set, reps x runs/100\n",
    "    tmp_MV = []#dummy for storing the above during repetition for Xruns\n",
    "    MSE_test = []#MSE on test set, reps x 1\n",
    "    WK1 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK1 = []#dummy during run\n",
    "    WK2 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK2 = []#dummy during run\n",
    "    WK3 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK3 = []#dummy during run\n",
    "    WW = []# Read Out Weights - store best weights, reps x s x s\n",
    "    tmp_WW = []#dummy during run\n",
    "    WM = []# Mask\n",
    "    tmp_WM = []\n",
    "    FEV = []#fraction of explained variance, reps x 1\n",
    "\n",
    "    #calculate test variance\n",
    "    gt_test_var = np.sum(np.var(GT_test,axis =1))#explainable output variance\n",
    "    \n",
    "    #initialize current attributes\n",
    "    lr=init_lr\n",
    "    # flags for early stopping\n",
    "    stop_flag = 0\n",
    "    sstop=0\n",
    "\n",
    "    #Init Mask weights\n",
    "    #STA - maximum pixel, use the smoothed STA, better estimation, see above!\n",
    "    tmp = np.random.normal(0,init_scaleM,tmp_sta.shape)\n",
    "    tmp[np.arange(N),np.argmax(abs(tmp_sta),1)] = np.ones(N)\n",
    "    init_mask = tmp.astype(np.float32).T\n",
    "    \n",
    "    #factorization\n",
    "    #tmpx=np.random.normal(0,init_scaleM,[N,s[2]])\n",
    "    #tmpy=np.random.normal(0,init_scaleM,[N,s[2]])\n",
    "    #dumx,dumy=np.unravel_index(np.argmax(abs(tmp_sta),1),(s[2],s[2]))\n",
    "    #tmpx[np.arange(N),dumx]=np.ones(N)\n",
    "    #init_maskx = tmpx.astype(np.float32)\n",
    "    #tmpy[np.arange(N),dumy]=np.ones(N)\n",
    "    #init_masky = tmpy.astype(np.float32)\n",
    "    #print(loc_x[:10],dumx[:10],loc_y[:10],dumy[:10])\n",
    "    \n",
    "    #init Weights\n",
    "    init_weights = (np.ones([num_kern[-1],N])*init_scaleW[0]).astype(np.float32)\n",
    "    if init_scaleW[1]>0:\n",
    "        init_weights = np.random.normal(init_scaleW[0],init_scaleW[1],[num_kern[-1],N])\n",
    "    \n",
    "    #Init model class\n",
    "    model = ModelGraph(s,rM,rW,init_scaleK,init_scaleM,init_scaleW,\n",
    "                       N,num_kern,init_mask,init_weights=init_weights)#,init_kernel)\n",
    "\n",
    "    #validation feed can be outside loop:\n",
    "    #feed_val = {model.X:X_val,\n",
    "    #            model.Y:Y_val,model.istrain:False}\n",
    "    #feed_test = {model.X:X_test,model.Y:GT_test,model.istrain:False}\n",
    "    \n",
    "    #Split up because too large for GPU memory...\n",
    "    num_test=np.int(GT_test.shape[1]/batch_size)\n",
    "    feed_test=[]\n",
    "    for i in range(num_test):\n",
    "        feed_test.append( {model.X:X_test[i*batch_size:(i+1)*batch_size,:,:,:],\n",
    "                        model.Y:GT_test[:,i*batch_size:(i+1)*batch_size],model.istrain:False})\n",
    "    num_val=np.int(Y_val.shape[1]/batch_size)\n",
    "    feed_val=[]\n",
    "    for i in range(num_val):\n",
    "        feed_val.append( {model.X:X_val[i*batch_size:(i+1)*batch_size,:,:,:],\n",
    "                        model.Y:Y_val[:,i*batch_size:(i+1)*batch_size],model.istrain:False})\n",
    "    \n",
    "    ##Start a tf session\n",
    "    with model.graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            #plot initial weights\n",
    "            if verbose:\n",
    "                print('Before Training:')\n",
    "                fig, ax = plt.subplots(1, 4, figsize=[18, 3])\n",
    "                #tmp_wk=model.WK1.eval().reshape([s[1],s[1],3,\n",
    "                #     num_kern[0]])[:,:,0,:].reshape([s2**2,num_kern[0]]).T\n",
    "                b=-1\n",
    "                tmp_ww=model.WW.eval()\n",
    "                tmp_wm=model.WM.eval()[:,0].T#first neuron as example\n",
    "                #for b in range(num_kern[0]):\n",
    "                #    ax[b].imshow(tmp_wk[b,:].reshape([s[1],s[1]]),cmap='bwr',\n",
    "                #                vmin=-max(abs(tmp_wk[b,:])),vmax=max(abs(tmp_wk[b,:])))\n",
    "                #    ax[b].get_xaxis().set_visible(False)\n",
    "                #    ax[b].get_yaxis().set_visible(False)\n",
    "                #    ax[b].set_title('Kernel')\n",
    "                ax[1+b].plot(tmp_ww[:,0])\n",
    "                ax[1+b].set_title('Weights N_1')\n",
    "                ax[2+b].imshow(tmp_wm.reshape([s[2],s[2]]),cmap='bwr',\n",
    "                    vmin=-max(abs(tmp_wm.T)),vmax=max(abs(tmp_wm.T)))\n",
    "                ax[2+b].get_xaxis().set_visible(False)\n",
    "                ax[2+b].get_yaxis().set_visible(False)\n",
    "                ax[2+b].set_title('Mask N_1')\n",
    "                test=np.zeros(num_test*batch_size)\n",
    "                for i in range(num_test):\n",
    "                    test[i*batch_size:(i+1)*batch_size]=sess.run(model.Y_,feed_test[i])[0]\n",
    "                ax[3+b].plot(GT_test[0], test, '.')\n",
    "                #ax[3+b].plot(GT_test[0], sess.run(model.Y_,feed_test)[0], '.')\n",
    "                xx = [-.4, .4]\n",
    "                ax[3+b].plot(xx, xx)\n",
    "                ax[3+b].axis('equal')\n",
    "                ax[3+b].set_title('true vs predicted')\n",
    "                ax[4+b].plot(tmp_ww)\n",
    "                ax[4+b].set_title('All Weights')\n",
    "                plt.show()\n",
    "                print('Loss: %s mask'%(model.regM.eval()*rM),\n",
    "                      'Loss: %s weights'%(model.regW.eval()*rW))\n",
    "\n",
    "            #Batches - define list of starting-indices for individual batches in data set:\n",
    "            #if there is less training data than batch size\n",
    "            batch_size = np.min([batch_size,X_train.shape[0]])\n",
    "            batch_ind = np.arange(0,X_train.shape[0],batch_size)\n",
    "            #number of selected batch\n",
    "            batch = 0\n",
    "\n",
    "            #Optimization runs\n",
    "            #minimal verbose\n",
    "            #verb_time=0\n",
    "            #from IPython import display\n",
    "            #fig,ax=plt.subplots(1,2, figsize=[10, 5])\n",
    "            for j in range(1,max_runs):\n",
    "\n",
    "                #when there is no further complete batch\n",
    "                if batch==len(batch_ind):\n",
    "                    #shuffle data and start again:\n",
    "                    ind = np.random.permutation(X_train.shape[0])\n",
    "                    X_train = X_train[ind,:,:,:]\n",
    "                    Y_train = Y_train[:,ind]\n",
    "                    batch = 0\n",
    "\n",
    "                #take a batch\n",
    "                X_batch = X_train[batch_ind[batch]:batch_ind[batch]+batch_size,:,:,:]\n",
    "                Y_batch = Y_train[:,batch_ind[batch]:batch_ind[batch]+batch_size]\n",
    "                batch +=1\n",
    "                \n",
    "                #Training feed:\n",
    "                feed_dict ={model.step_size:lr,model.X:X_batch,\n",
    "                            model.Y:Y_batch,model.istrain:True}\n",
    "\n",
    "                # Training with current batch:\n",
    "                sess.run([model.training, model.update_ops],feed_dict)\n",
    "                \n",
    "                #Early Stopping - check if MSE doesn't increase\n",
    "                if j%100==0:\n",
    "\n",
    "                    model.saver.save(sess, 'Batty_bn_checkpoint', global_step=int(j/100))\n",
    "\n",
    "                    #exponentially decreasing learning rate?\n",
    "                    #lr *= .99\n",
    "\n",
    "                    # Store MSE on train:\n",
    "                    \n",
    "                    tmp_MT.append(model.MSE.eval(feed_dict))\n",
    "\n",
    "                    #check MSE on validation set and store the parameters\n",
    "                    #tmp_MV.append(model.MSE.eval(feed_val))\n",
    "                    val=np.zeros(num_val)\n",
    "                    for i in range(num_val):\n",
    "                        val[i]=sess.run(model.MSE,feed_val[i])\n",
    "                    tmp_MV.append(np.mean(val))\n",
    "                    tmp_WK1.append(model.WK1.eval())\n",
    "                    tmp_WK2.append(model.WK2.eval())\n",
    "                    tmp_WK3.append(model.WK3.eval())\n",
    "                    tmp_WW.append(model.WW.eval())\n",
    "                    tmp_WM.append(model.WM.eval())\n",
    "\n",
    "                    #Best run\n",
    "                    if len(tmp_MV)>5:#burn in \n",
    "                        tmp_min_ind = np.argmin(tmp_MV[5:])+5\n",
    "                    else:\n",
    "                        tmp_min_ind = len(tmp_MV)-1\n",
    "                        \n",
    "                    #minimal verbose\n",
    "                    #start=time.time()\n",
    "                    #print('run=%s,lr=%s,MSE-train=%s,MSE-val=%s'%(j,lr,\n",
    "                    #            tmp_MT[-1],tmp_MV[-1]))\n",
    "                    if j>99100:\n",
    "                        #test=np.zeros(num_test)\n",
    "                        #for i in range(num_test):\n",
    "                        #    test[i]=sess.run(model.MSE,feed_test[i])\n",
    "                        #MSE_test = np.mean(test)\n",
    "                        \n",
    "                        ax[0].plot(np.arange(len(tmp_MT)-1),tmp_MT[1:],'r')\n",
    "                        ax[0].plot(np.arange(len(tmp_MT)-1),tmp_MV[1:],'g')\n",
    "                        ax[0].legend(['Train MSE','Val MSE'])\n",
    "                        #ax[0].legend(['Val MSE'])\n",
    "                        ax[0].set_title('run=%s,lr=%s,plot_time=%s,MSE-train=%s,MSE-val=%s'%(j,\n",
    "                                      lr,np.round(verb_time),tmp_MT[-1],tmp_MV[-1]))\n",
    "                        #ax[1].set_title('MSE-val=%s, FEV=%s'%(tmp_MV[-1],\n",
    "                        #        (1 - (MSE_test/gt_test_var))))\n",
    "                        \n",
    "                    #verb_time+=time.time()-start\n",
    "\n",
    "                    ##Analytics - Display progress\n",
    "                    if verbose:\n",
    "                        #to calculate FEV\n",
    "                        test=np.zeros(num_test)\n",
    "                        for i in range(num_test):\n",
    "                            test[i]=sess.run(model.MSE,feed_test[i])\n",
    "                        MSE_gt = np.mean(test)\n",
    "                        print('Runs: %s; MSE - train: %s, val: %s; lr = %s'%\n",
    "                              (j,tmp_MT[-1],tmp_MV[-1],lr))\n",
    "                        print('Loss: %s mask'%(model.regM.eval()*rM),\n",
    "                              'Loss: %s weights'%(model.regW.eval()*rW))\n",
    "                        print('latest: ',tmp_MV[-1])\n",
    "                        print('FEV = ',(1 - (MSE_gt/gt_test_var)))\n",
    "                        #print(len(tmp_MV),tmp_min_ind)\n",
    "                        print('best: ',tmp_min_ind,tmp_MV[tmp_min_ind])\n",
    "\n",
    "                        #Plot learned weight example\n",
    "                        fig, ax = plt.subplots(1, 5, figsize=[18, 3])\n",
    "                        #tmp_wk=model.WK1.eval().reshape([s[1],s[1],3,\n",
    "                        # num_kern[0]])[:,:,0,:].reshape([s[1]**2,num_kern[0]]).T\n",
    "                        tmp_ww=model.WW.eval()\n",
    "                        tmp_wm=model.WM.eval()[:,0].T#first neuron as example\n",
    "                        #for b in range(num_kern[0]):\n",
    "                        #    ax[b].imshow(tmp_wk[b,:].reshape([s[1],s[1]]),cmap='bwr',\n",
    "                        #                vmin=-max(abs(tmp_wk[b,:])),vmax=max(abs(tmp_wk[b,:])))\n",
    "                        #    ax[b].get_xaxis().set_visible(False)\n",
    "                        #    ax[b].get_yaxis().set_visible(False)\n",
    "                        #    ax[b].set_title('Kernel')\n",
    "                        b=-1\n",
    "                        ax[1+b].plot(tmp_ww[:,0])\n",
    "                        ax[1+b].set_title('Weights N_1')\n",
    "                        ax[2+b].imshow(tmp_wm.reshape([s3,s3]),cmap='bwr',\n",
    "                            vmin=-max(abs(tmp_wm.T)),vmax=max(abs(tmp_wm.T)))\n",
    "                        ax[2+b].get_xaxis().set_visible(False)\n",
    "                        ax[2+b].get_yaxis().set_visible(False)\n",
    "                        ax[2+b].set_title('Mask N_1')\n",
    "                        ax[2+b].axhline(y=loc_y[0]+1, xmin=0, xmax=32, linewidth=1, color = 'g')\n",
    "                        ax[2+b].axvline(x=loc_x[0]-1, ymin=0, ymax =32, linewidth=1, color='g')\n",
    "                        test=np.zeros(num_test*batch_size)\n",
    "                        for i in range(num_test):\n",
    "                            test[i*batch_size:(i+1)*batch_size]=sess.run(model.Y_,feed_test[i])[0]\n",
    "                        ax[3+b].plot(GT_test[0], test, '.')\n",
    "                        #ax[3+b].plot(GT_test[0], sess.run(model.Y_,feed_test)[0], '.')\n",
    "                        xx = [-.4, .4]\n",
    "                        ax[3+b].plot(xx, xx)\n",
    "                        ax[3+b].axis('equal')\n",
    "                        ax[3+b].set_title('true vs predicted')\n",
    "                        ax[4+b].plot(tmp_ww[:,:250],'r')\n",
    "                        ax[4+b].plot(tmp_ww[:,250:500],'b')\n",
    "                        ax[4+b].plot(tmp_ww[:,500:750],'y')\n",
    "                        ax[4+b].plot(tmp_ww[:,750:],'g')\n",
    "                        ax[4+b].set_title('All Weights')\n",
    "                        ax[5+b].plot(tmp_MV)\n",
    "                        ax[5+b].plot(tmp_MT)\n",
    "                        ax[5+b].set_ylim([min(tmp_MT),2*np.median(tmp_MV)-min(tmp_MT)])\n",
    "                        ax[5+b].legend(['MSE Val','MSE Train'])\n",
    "                        plt.show()\n",
    "\n",
    "                    ##Early Stopping - if latest validation MSE is not minimum\n",
    "                    if tmp_min_ind != len(tmp_MV)-1:\n",
    "                        stop_flag +=1\n",
    "                        if stop_flag>=8:\n",
    "                            lr *= .1\n",
    "                            #set back to previous best?\n",
    "                            #model.saver.restore(sess, 'bn_checkpoint-%s'%(tmp_min_ind+1))\n",
    "                            #print('back to MSE-val = ',model.MSE_test.eval(feed_val))\n",
    "                            stop_flag = 0\n",
    "                            sstop +=1\n",
    "                            if sstop==3:#lower the lr x times\n",
    "                                break\n",
    "                                \n",
    "                    else:#if latest value is best, reset\n",
    "                        stop_flag = 0\n",
    "\n",
    "            #Best run\n",
    "            tmp_min_ind = np.argmin(tmp_MV)\n",
    "\n",
    "            #Store MSEs\n",
    "            MSE_train = tmp_MT#list\n",
    "            MSE_val = tmp_MV#list\n",
    "            \n",
    "            #Store best weights (i.e. lowest validation MSE)\n",
    "            WK1 = tmp_WK1[tmp_min_ind]#s2 x s2\n",
    "            WK2 = tmp_WK2[tmp_min_ind]#s2 x s2\n",
    "            WK3 = tmp_WK3[tmp_min_ind]#s2 x s2\n",
    "            WW = tmp_WW[tmp_min_ind]# N x s*s\n",
    "            WM = tmp_WM[tmp_min_ind]# num_kern x N\n",
    "\n",
    "            #Assign the best weights to model graph \n",
    "            model.saver.restore(sess, './Batty_bn_checkpoint-%s'%(tmp_min_ind+1))\n",
    "            \n",
    "            #clean checkpoints\n",
    "            files = os.listdir()\n",
    "            for file in files:\n",
    "                if file.startswith(\"Batty_bn_checkpoint\"):\n",
    "                    os.remove(file)\n",
    "                    \n",
    "            # Test MSE prediction\n",
    "            #MSE_test = sess.run(model.MSE,feed_test)#1 x 1\n",
    "            test=np.zeros(num_test)\n",
    "            #test_L=np.zeros(num_test)\n",
    "            for i in range(num_test):\n",
    "                test[i]=sess.run(model.MSE,feed_test[i])\n",
    "            #    test_L[i]=sess.run(model.poisson,feed_test[i])\n",
    "            MSE_test = np.mean(test)\n",
    "            #Loss_test = np.mean(Loss)\n",
    "            \n",
    "            #FEV - fraction of explainable variance\n",
    "            FEV = 1 - (MSE_test/gt_test_var)#1 x 1\n",
    "\n",
    "            #Predicted outputs\n",
    "            #Y_ = sess.run(model.Y_,feed_test)\n",
    "            test=np.zeros([N,num_test*batch_size])\n",
    "            for i in range(num_test):\n",
    "                test[:,i*batch_size:(i+1)*batch_size]=sess.run(model.Y_,feed_test[i])\n",
    "            Y_ = test\n",
    "            \n",
    "            #FEV per cell\n",
    "            fev_cell=1-(np.mean((Y_-GT_test)**2,1)/np.var(GT_test,1))\n",
    "\n",
    "            #Output\n",
    "            log=('Stop at run %s; MSE on validation set: %s'% (j,MSE_val[tmp_min_ind]),\n",
    "                  'MSE on test set: %s; FEV: %s' % (MSE_test, FEV))\n",
    "            print(log)\n",
    "\n",
    "    return (WK1,WK2,WK3,WM,WW,MSE_train,MSE_val,MSE_test,FEV,fev_cell,Y_,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4,8,16 Types (64 each) - D=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for Training: 3584, Validation: 512, Testing 1024\n",
      "it took 10.098278760910034 s to preprocess data\n",
      "('Stop at run 4500; MSE on validation set: 32.57589221', 'MSE on test set: 6.90853881836; FEV: 0.603982150429')\n",
      "('Stop at run 4500; MSE on validation set: 33.2664337158', 'MSE on test set: 7.72155472636; FEV: 0.557377677324')\n",
      "('Stop at run 4800; MSE on validation set: 32.553155899', 'MSE on test set: 6.82787340879; FEV: 0.608606129952')\n",
      "('Stop at run 4800; MSE on validation set: 33.4574923515', 'MSE on test set: 7.83693635464; FEV: 0.550763661609')\n",
      "('Stop at run 5400; MSE on validation set: 34.4684443474', 'MSE on test set: 8.92171046138; FEV: 0.488581205909')\n",
      "('Stop at run 5100; MSE on validation set: 62.5602178574', 'MSE on test set: 10.4106494784; FEV: 0.623441029694')\n",
      "('Stop at run 4000; MSE on validation set: 62.174826622', 'MSE on test set: 10.0574992895; FEV: 0.636214668051')\n",
      "('Stop at run 5700; MSE on validation set: 61.9469623566', 'MSE on test set: 9.83836752176; FEV: 0.644140785725')\n",
      "('Stop at run 4800; MSE on validation set: 62.2354269028', 'MSE on test set: 10.1803237796; FEV: 0.631772038069')\n",
      "('Stop at run 3600; MSE on validation set: 75.9460248947', 'MSE on test set: 24.808381319; FEV: 0.102667077225')\n",
      "('Stop at run 4600; MSE on validation set: 126.962878227', 'MSE on test set: 26.1706360579; FEV: 0.533461405939')\n",
      "('Stop at run 4400; MSE on validation set: 126.657678604', 'MSE on test set: 25.6605893373; FEV: 0.542553904851')\n",
      "('Stop at run 4700; MSE on validation set: 126.601851463', 'MSE on test set: 25.6025556326; FEV: 0.543588459876')\n",
      "('Stop at run 4900; MSE on validation set: 127.373066902', 'MSE on test set: 26.7235955; FEV: 0.523603910686')\n",
      "('Stop at run 3300; MSE on validation set: 148.413692474', 'MSE on test set: 46.8004393578; FEV: 0.165698107943')\n",
      "('Stop at run 4600; MSE on validation set: 32.4892847538', 'MSE on test set: 6.89415004849; FEV: 0.604806957215')\n",
      "('Stop at run 4600; MSE on validation set: 32.5244119167', 'MSE on test set: 6.95841562748; FEV: 0.601123064418')\n",
      "('Stop at run 4700; MSE on validation set: 32.6680707932', 'MSE on test set: 6.95872858167; FEV: 0.601105124959')\n",
      "('Stop at run 4400; MSE on validation set: 33.8615396023', 'MSE on test set: 8.20964595675; FEV: 0.529398846411')\n",
      "('Stop at run 6200; MSE on validation set: 34.8285365105', 'MSE on test set: 9.26938438416; FEV: 0.468651509794')\n",
      "('Stop at run 5200; MSE on validation set: 61.8153924942', 'MSE on test set: 9.8961982429; FEV: 0.642049016441')\n",
      "('Stop at run 4900; MSE on validation set: 62.0497136116', 'MSE on test set: 9.8540148735; FEV: 0.643574812327')\n",
      "('Stop at run 5000; MSE on validation set: 62.2251958847', 'MSE on test set: 10.2492171526; FEV: 0.629280126526')\n",
      "('Stop at run 5200; MSE on validation set: 61.8051371574', 'MSE on test set: 9.71866765618; FEV: 0.648470396305')\n",
      "('Stop at run 9200; MSE on validation set: 67.1806769371', 'MSE on test set: 15.4203034639; FEV: 0.442239064314')\n",
      "('Stop at run 4200; MSE on validation set: 126.568778992', 'MSE on test set: 25.8974123001; FEV: 0.538332110171')\n",
      "('Stop at run 5000; MSE on validation set: 127.147397995', 'MSE on test set: 26.21169734; FEV: 0.532729414832')\n",
      "('Stop at run 4700; MSE on validation set: 126.885700226', 'MSE on test set: 26.0252604485; FEV: 0.536052987293')\n",
      "('Stop at run 5600; MSE on validation set: 126.609747887', 'MSE on test set: 25.7565529346; FEV: 0.540843181365')\n",
      "('Stop at run 9400; MSE on validation set: 151.814163208', 'MSE on test set: 51.0924472809; FEV: 0.0891853576328')\n",
      "('Stop at run 4700; MSE on validation set: 32.6509919167', 'MSE on test set: 6.92118561268; FEV: 0.603257198826')\n",
      "('Stop at run 4700; MSE on validation set: 32.9771895409', 'MSE on test set: 7.2730383575; FEV: 0.583087960289')\n",
      "('Stop at run 4800; MSE on validation set: 32.6045944691', 'MSE on test set: 6.80263704062; FEV: 0.610052753112')\n",
      "('Stop at run 4600; MSE on validation set: 33.7875277996', 'MSE on test set: 8.10068172216; FEV: 0.535644998165')\n",
      "('Stop at run 6900; MSE on validation set: 38.5004448891', 'MSE on test set: 13.2867625356; FEV: 0.238363528751')\n",
      "('Stop at run 5100; MSE on validation set: 61.9294309616', 'MSE on test set: 9.83886432648; FEV: 0.644122816023')\n",
      "('Stop at run 4700; MSE on validation set: 61.7670927048', 'MSE on test set: 9.66303005815; FEV: 0.650482839109')\n",
      "('Stop at run 5000; MSE on validation set: 62.2270460129', 'MSE on test set: 9.97471129894; FEV: 0.639209155623')\n",
      "('Stop at run 5200; MSE on validation set: 62.4933943748', 'MSE on test set: 10.3342728019; FEV: 0.626203616479')\n",
      "('Stop at run 11700; MSE on validation set: 66.9922847748', 'MSE on test set: 14.8025635481; FEV: 0.464583059961')\n",
      "('Stop at run 4700; MSE on validation set: 126.972054482', 'MSE on test set: 25.9907063246; FEV: 0.536668976615')\n",
      "('Stop at run 4300; MSE on validation set: 127.183739662', 'MSE on test set: 26.0977412462; FEV: 0.534760886886')\n",
      "('Stop at run 4300; MSE on validation set: 125.829010963', 'MSE on test set: 24.9511529207; FEV: 0.55520088323')\n",
      "('Stop at run 5200; MSE on validation set: 127.058496475', 'MSE on test set: 26.0270177126; FEV: 0.536021660903')\n",
      "('Stop at run 4300; MSE on validation set: 152.267566681', 'MSE on test set: 51.5779750347; FEV: 0.0805299533412')\n",
      "('Stop at run 4900; MSE on validation set: 32.599722147', 'MSE on test set: 6.87330192327; FEV: 0.606002033328')\n",
      "('Stop at run 4800; MSE on validation set: 32.6603469849', 'MSE on test set: 6.94901704788; FEV: 0.601661818759')\n",
      "('Stop at run 5300; MSE on validation set: 32.7639906406', 'MSE on test set: 7.04264506698; FEV: 0.596294784172')\n",
      "('Stop at run 4100; MSE on validation set: 33.7393753529', 'MSE on test set: 8.02483049035; FEV: 0.539993014801')\n",
      "('Stop at run 3800; MSE on validation set: 34.368342638', 'MSE on test set: 8.83468586206; FEV: 0.493569712971')\n",
      "('Stop at run 4800; MSE on validation set: 61.9018058777', 'MSE on test set: 9.81548935175; FEV: 0.644968301833')\n",
      "('Stop at run 4300; MSE on validation set: 61.9461317062', 'MSE on test set: 10.1502487063; FEV: 0.632859870162')\n",
      "('Stop at run 4800; MSE on validation set: 62.0149998665', 'MSE on test set: 10.108674109; FEV: 0.634363646425')\n",
      "('Stop at run 4600; MSE on validation set: 62.1522893906', 'MSE on test set: 10.0969590545; FEV: 0.63478738645')\n",
      "('Stop at run 10100; MSE on validation set: 67.9467926025', 'MSE on test set: 15.9720726609; FEV: 0.422281266187')\n",
      "('Stop at run 5300; MSE on validation set: 127.589694023', 'MSE on test set: 26.6502871513; FEV: 0.524910763673')\n",
      "('Stop at run 4700; MSE on validation set: 127.21291256', 'MSE on test set: 26.3492196798; FEV: 0.530277832115')\n",
      "('Stop at run 5100; MSE on validation set: 126.221894264', 'MSE on test set: 25.3241219521; FEV: 0.548552040339')\n",
      "('Stop at run 5200; MSE on validation set: 126.81503582', 'MSE on test set: 26.1697161198; FEV: 0.533477805488')\n",
      "('Stop at run 4500; MSE on validation set: 151.795471191', 'MSE on test set: 51.0972578526; FEV: 0.0890996005523')\n",
      "('Stop at run 6000; MSE on validation set: 32.5679831505', 'MSE on test set: 6.7551510632; FEV: 0.612774789588')\n",
      "('Stop at run 4700; MSE on validation set: 32.7305715084', 'MSE on test set: 6.99278029799; FEV: 0.599153180007')\n",
      "('Stop at run 4500; MSE on validation set: 32.6084473133', 'MSE on test set: 6.82411167026; FEV: 0.608821763915')\n",
      "('Stop at run 5000; MSE on validation set: 32.7580916882', 'MSE on test set: 7.14904227853; FEV: 0.59019578176')\n",
      "('Stop at run 16900; MSE on validation set: 38.3353967667', 'MSE on test set: 12.8936026692; FEV: 0.260900613494')\n",
      "('Stop at run 4200; MSE on validation set: 62.1196160316', 'MSE on test set: 9.98511868715; FEV: 0.638832714615')\n",
      "('Stop at run 5100; MSE on validation set: 62.4246025085', 'MSE on test set: 10.3290685415; FEV: 0.626391857465')\n",
      "('Stop at run 4800; MSE on validation set: 62.0208268166', 'MSE on test set: 9.75548815727; FEV: 0.647138578343')\n",
      "('Stop at run 6700; MSE on validation set: 62.0899343491', 'MSE on test set: 10.10272789; FEV: 0.634578724466')\n",
      "('Stop at run 3000; MSE on validation set: 76.5354785919', 'MSE on test set: 25.4824906588; FEV: 0.0782841682279')\n",
      "('Stop at run 4400; MSE on validation set: 126.894042015', 'MSE on test set: 25.9848860502; FEV: 0.53677273346')\n",
      "('Stop at run 4900; MSE on validation set: 126.399662018', 'MSE on test set: 25.4328922033; FEV: 0.546613015243')\n",
      "('Stop at run 4700; MSE on validation set: 126.48651123', 'MSE on test set: 25.8200428486; FEV: 0.539711359611')\n",
      "('Stop at run 5200; MSE on validation set: 126.706430435', 'MSE on test set: 25.8783644438; FEV: 0.538671672423')\n",
      "('Stop at run 3000; MSE on validation set: 152.774841309', 'MSE on test set: 52.0624113083; FEV: 0.0718940066456')\n"
     ]
    }
   ],
   "source": [
    "### grid search\n",
    "reps = 5\n",
    "\n",
    "Neurons = [4*64,8*64,16*64]\n",
    "\n",
    "start=time.time()\n",
    "#Seeds\n",
    "np_seed=1234\n",
    "np.random.seed(np_seed)\n",
    "tf_seed=1234\n",
    "\n",
    "#Data, keep test set for later\n",
    "N=Neurons[-1]\n",
    "num_test = 2**10\n",
    "ind = np.random.choice(X.shape[0],X.shape[0],replace=False)\n",
    "X_TEST = X[ind[:num_test],...]\n",
    "GT_TEST = ((Y[:,ind[:num_test]].T / np.mean(Y,1)).T * .1)[:N,:]#mean response=.1\n",
    "X_try = X[ind[num_test:],...]\n",
    "Y_try = Y[:N,ind[num_test:]]\n",
    "\n",
    "#Data\n",
    "D=2**12\n",
    "split_data=True#set true for large nets to fit on GPU memory\n",
    "batch_size = 64\n",
    "D=min(D,Y_try.shape[1])#number of training+val images\n",
    "num_val = np.min([D//8,num_test])#limit validation set to test set size\n",
    "num_train = D-num_val\n",
    "\n",
    "Y_train,Y_val,X_train,X_val,X_test,GT_test = splitting_data(X=X_try,Y=Y_try,\n",
    "   num_train=num_train,num_test=num_test,num_val=num_val,noise=True,mean_response=.1)\n",
    "\n",
    "print('Images for Training: %s, Validation: %s, Testing %s'%(\n",
    "      Y_train.shape[1],Y_val.shape[1],GT_test.shape[1]))\n",
    "\n",
    "#Ground truth locations\n",
    "GT_mask = np.hstack([loc_y.reshape([-1,1]),loc_x.reshape([-1,1])])#[ind,:]\n",
    "\n",
    "#Parameters:\n",
    "s1=44#width=heigth of image\n",
    "s2=5\n",
    "s3=32\n",
    "n_B = [32,64,0]#number of bases to learn\n",
    "init_scaleK = .01#Kernel\n",
    "init_scaleM = .001#Mask\n",
    "init_scaleW = [0,.01]#Read out Weights,    1/n_B[-1]\n",
    "lr= .001 # initial learning rate\n",
    "rM = 0#.05#regularization on Mask\n",
    "rW = [.00001,.0001,.001,.01,.1]#tried:.005\n",
    "#loop parameters\n",
    "max_runs = 150000 # training steps\n",
    "batch_size = 64\n",
    "    \n",
    "#Clean previous checkpoints\n",
    "files = os.listdir()\n",
    "for file in files:\n",
    "    if file.startswith(\"Batty_bn_checkpoint\"):\n",
    "        os.remove(file)\n",
    "\n",
    "#calculate spike triggered average to initialize location masks\n",
    "X_sta=np.mean(X_train,axis=1).reshape([-1,s1**2]).T\n",
    "sta,_=STA(X_sta,Y_train,s3,s3//2)\n",
    "\n",
    "\n",
    "print('it took %s s to preprocess data'%(time.time()-start))\n",
    "\n",
    "Val_c4 = np.zeros([reps,5])#rep,reg (mean over neurons)\n",
    "Fev_c4 = np.zeros([reps,5,Neurons[0]])\n",
    "Val_c8 = np.zeros([reps,5])#rep,reg (mean over neurons)\n",
    "Fev_c8 = np.zeros([reps,5,Neurons[1]])\n",
    "Val_c16 = np.zeros([reps,5])#rep,reg (mean over neurons)\n",
    "Fev_c16 = np.zeros([reps,5,Neurons[2]])\n",
    "\n",
    "for rep in range(reps):\n",
    "    with open(\"batty_log.txt\", \"a\") as file:\n",
    "        print('repetition ',rep, file=file)\n",
    "    for n in range(len(Neurons)):\n",
    "        n_B[2] = Neurons[n]//64\n",
    "        init_scaleW[0] = 1/n_B[2]\n",
    "        with open(\"batty_log.txt\", \"a\") as file:\n",
    "            print('neurons %s, kernels/types %s'%(Neurons[n],n_B[2]),file=file)\n",
    "        for reg in range(len(rW)):\n",
    "            with open(\"batty_log.txt\", \"a\") as file:\n",
    "                print('regularization = ',rW[reg],file=file)\n",
    "    \n",
    "            (WK1,WK2,WK3,WM_tmp,WW_tmp,MSE_train,MSE_val,\n",
    "             MSE_test,FEV,fev_c,Y_tmp,log) = train(init_scaleK,\n",
    "                       init_scaleM,init_scaleW,lr,n_B,sta[:Neurons[n],:],\n",
    "                       max_runs,rM,rW[reg],[s1,s2,s3],Neurons[n],X_train,X_val,X_TEST,\n",
    "                       Y_train[:Neurons[n],:],Y_val[:Neurons[n],:],\n",
    "                       GT_TEST[:Neurons[n],:],batch_size,verbose=False)\n",
    "            \n",
    "            if n==0:\n",
    "                Val_c4[rep,reg] = np.min(MSE_val)\n",
    "                Fev_c4[rep,reg,:] = fev_c\n",
    "\n",
    "            if n==1:\n",
    "                Val_c8[rep,reg] = np.min(MSE_val)\n",
    "                Fev_c8[rep,reg,:] = fev_c\n",
    "\n",
    "            if n==2:\n",
    "                Val_c16[rep,reg] = np.min(MSE_val)\n",
    "                Fev_c16[rep,reg,:] = fev_c\n",
    "\n",
    "            #saving text output to log file\n",
    "            with open(\"batty_log.txt\", \"a\") as file:\n",
    "                print(log, file=file)\n",
    "            np.savez('batty_4-8-16_types',\n",
    "                     Val_c4=Val_c4,\n",
    "                     Fev_c4=Fev_c4,\n",
    "                     Val_c8=Val_c8,\n",
    "                     Fev_c8=Fev_c8,\n",
    "                     Val_c16=Val_c16,\n",
    "                     Fev_c16=Fev_c16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
