{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Setup:\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns#nicer plots?\n",
    "sns.reset_orig()\n",
    "import scipy.stats as sps\n",
    "from scipy.stats import chi\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "fig_size = [18,18]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# Tensorflow needs to be LAST import\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T11:19:37.668473",
     "start_time": "2017-09-08T11:19:34.019949"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Loading the data from VGG Conv2 activations\n",
    "\n",
    "Data=np.load('/gpfs01/bethge/home/dklindt/David/publish/fig5/more_types/data_all.npz')\n",
    "\n",
    "print(Data.keys())\n",
    "Y=Data.f.responses\n",
    "Y=Y.T# N x D\n",
    "X=Data.f.images\n",
    "X=np.transpose(X,axes=[0,3,1,2])# NCHW\n",
    "loc_y=Data.f.loc_y\n",
    "loc_x=Data.f.loc_x\n",
    "feature_maps=Data.f.feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:57:43.962771",
     "start_time": "2017-09-08T10:57:43.917135"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! splits data into train,val,test and adds poisson-like noise\n",
    "def splitting_data(X,Y,num_train,num_val,num_test=2**10,noise=True,mean_response=.1):\n",
    "    \n",
    "    N=Y.shape[0]#number of neurons\n",
    "    \n",
    "    X_train=X[:num_train,:,:,:]\n",
    "    X_val=X[num_train:num_train+num_val,:,:,:]\n",
    "    X_test=X[num_train+num_val:num_train+num_val+num_test,:,:,:]\n",
    "    \n",
    "    Y_train=np.zeros([N,num_train])\n",
    "    Y_val=np.zeros([N,num_val])\n",
    "    #Y_test=np.zeros([N,num_test])\n",
    "    GT_test=np.zeros([N,num_test])\n",
    "    \n",
    "    for n in range(1000):\n",
    "        tmp_mean = np.mean(Y[n,:])\n",
    "        Y_train[n,:] = Y[n,:num_train] / tmp_mean * mean_response\n",
    "        Y_val[n,:] = Y[n,num_train:num_train+num_val] / tmp_mean * mean_response\n",
    "        GT_test[n,:] = Y[n,num_train+num_val:num_train+num_val+num_test] / tmp_mean * mean_response\n",
    "    \n",
    "    #Poisson-like noise\n",
    "    if noise:\n",
    "        Y_train += np.random.normal(0,np.sqrt(np.abs(Y_train)),Y_train.shape)\n",
    "        Y_val +=  np.random.normal(0,np.sqrt(np.abs(Y_val)),Y_val.shape)\n",
    "        #Y_test =  GT_test + np.random.normal(0,np.sqrt(np.abs(GT_test)),GT_test.shape)\n",
    "    \n",
    "    #Poisson noise\n",
    "    #if noise:\n",
    "    #    Y_train += np.random.poisson(Y_train)\n",
    "    #    Y_val +=  np.random.poisson(Y_val)\n",
    "    #    Y_test =  GT_test + np.random.poisson(GT_test)\n",
    "    \n",
    "    return Y_train,Y_val,X_train,X_val,X_test,GT_test#,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:58:00.258858",
     "start_time": "2017-09-08T10:58:00.165292"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Spike triggered average - as initialization or else\n",
    "def STA(X,Y,crop,s2=13):\n",
    "    #In:\n",
    "    #X - stimuli, stimulus_size x number_of_data\n",
    "    #Y - responses, number_of_neurons x number_of_data\n",
    "    #smooth - size of smoothing gaussian (=s2), if not provided, no smoothing\n",
    "    #Out\n",
    "    #sta - spike triggered average, number_of_neurons x stimulus_size\n",
    "    s=np.sqrt(X.shape[0]).astype(int)\n",
    "    d=X.shape[1]\n",
    "    n=Y.shape[0]\n",
    "    \n",
    "    sta = ((X.dot(Y.T))/Y.shape[1]).T\n",
    "    sta = sta.reshape([n,s*s])\n",
    "    \n",
    "    #Smoothing\n",
    "    x = np.linspace(1, s2, s2)\n",
    "    y = np.linspace(s2, 1, s2)\n",
    "    xm, ym = np.meshgrid(x, y)\n",
    "\n",
    "    centre = [s2/2+.5, s2/2+.5]\n",
    "    ind_tmp = (np.abs(xm-centre[0]) < s2) & (np.abs(ym-centre[1]) < s2)\n",
    "    rf_tmp = np.zeros((s2,s2))\n",
    "    rf_tmp[ind_tmp] = np.sqrt( (centre[0] - xm[ind_tmp])**2 +\n",
    "                      (centre[1] - ym[ind_tmp])**2 )\n",
    "    rf_tmp[ind_tmp] = (sps.norm.pdf(rf_tmp[ind_tmp],0,s2**(1/4)))\n",
    "\n",
    "    normal=rf_tmp\n",
    "    #plt.imshow(normal)\n",
    "    #plt.show()\n",
    "    sta_smooth=np.zeros(sta.shape)\n",
    "    sta_r1=np.zeros([n,s,s])\n",
    "    for i in range(n):\n",
    "        sta_smooth[i,:] = signal.convolve2d(((sta[i,:])**2).reshape([s,s]),\n",
    "            normal,mode='same').reshape(s**2)\n",
    "        #rank one approx\n",
    "        U,S,V = np.linalg.svd(sta[i,:].reshape(s,s))\n",
    "        S1 = np.zeros(S.shape)\n",
    "        S1[0] = S[0]\n",
    "        tmp1 = U.dot(np.diag(S1).dot(V))\n",
    "        sta_r1[i,:,:] = signal.convolve2d((tmp1**2),\n",
    "            normal,mode='same')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #cropping\n",
    "    ind = np.int((s-crop)/2)\n",
    "    sta = sta.reshape([n,s,s])\n",
    "    sta = sta[:,ind:s-ind,ind:s-ind]\n",
    "    sta = sta.reshape([n,crop**2])\n",
    "    sta_s = sta_smooth.reshape([n,s,s])\n",
    "    sta_s = sta_s[:,ind:s-ind,ind:s-ind]\n",
    "    sta_s = sta_s.reshape([n,crop**2])\n",
    "    sta_r1 = sta_r1[:,ind:s-ind,ind:s-ind]\n",
    "    sta_r1 = sta_r1.reshape([n,crop**2])\n",
    "        \n",
    "    return sta, sta_s#, sta_r1\n",
    "\n",
    "\n",
    "#######rank one approx\n",
    "'''\n",
    "for i in range(3):\n",
    "    tmp = tmp_sta[356+i,:].reshape(32,32).copy()\n",
    "    plt.imshow(tmp)\n",
    "    plt.show()\n",
    "    u,s,v = np.linalg.svd(tmp)\n",
    "    print(u.shape,s.shape,v.shape)\n",
    "    s1 = np.zeros(s.shape)\n",
    "    s1[0] = s[0]\n",
    "    tmp1 = u.dot(np.diag(s1).dot(v))\n",
    "    plt.imshow(tmp1)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:58:04.572442",
     "start_time": "2017-09-08T10:58:03.900034"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Model tensorflow - mask: trainable:False\n",
    "from tensorflow.contrib import layers\n",
    "class ModelGraph:\n",
    "    def __init__(self, s, rM, rW, init_scaleK, init_scaleM,init_scaleW, N, num_kern,\n",
    "                 init_mask=np.array([]),\n",
    "                 init_weights=np.array([]), init_kernel=np.array([])):\n",
    "        #Inputs:\n",
    "        #        s*s - size of the image\n",
    "        #        s2*s2 - size of the kernel\n",
    "        #        rM/W - regularization weight Mask / Weights\n",
    "        #        N - number of neurons\n",
    "        #        num_kern - number of kernels per conv layer\n",
    "        \n",
    "        self.graph = tf.Graph()#new tf graph\n",
    "        with self.graph.as_default():#use it as default\n",
    "        \n",
    "            #input tensor of shape NCHW!\n",
    "            self.X = tf.placeholder(tf.float32,shape=[None,3,s[0],s[0]])\n",
    "            #output: N x None\n",
    "            self.Y = tf.placeholder(tf.float32)             \n",
    "            \n",
    "            #WK Kernel - filter / tensor of shape H-W-InChannels-OutChannels\n",
    "                \n",
    "            #WW - Read Out Weights\n",
    "            if init_weights.size:\n",
    "                self.WW_init = init_weights\n",
    "            else:\n",
    "                self.WW_init = tf.random_normal([num_kern[-1],N],\n",
    "                                                init_scaleW[0],init_scaleW[1])\n",
    "            \n",
    "            #WM - Mask\n",
    "            if init_mask.size:\n",
    "                self.WM_init = init_mask\n",
    "                #self.WM_initx = init_maskx\n",
    "                #self.WM_inity = init_masky\n",
    "            else:\n",
    "                self.WM_init = tf.random_normal([s[2]**2,N],0,init_scaleM)\n",
    "                #self.WM_initx = tf.random_normal([N,s[2]],0,init_scaleM)\n",
    "                #self.WM_inity = tf.random_normal([N,s[2]],0,init_scaleM)\n",
    "        \n",
    "            #batch normalization settings\n",
    "            self.istrain = tf.placeholder(tf.bool)\n",
    "            \n",
    "            bn_params = dict(center=True,\n",
    "                             scale=False,\n",
    "                             is_training=self.istrain,\n",
    "                             variables_collections=['batch_norm_ema'])\n",
    "            \n",
    "            #Layer: Conv1\n",
    "            self.conv1 = layers.convolution2d(\n",
    "                inputs=self.X,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[0],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv1')\n",
    "            with tf.variable_scope('conv1', reuse=True):\n",
    "                self.WK1 = tf.get_variable('weights')\n",
    "                \n",
    "            #Layer: Conv2\n",
    "            self.conv2 = layers.convolution2d(\n",
    "                inputs=self.conv1,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[1],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv2')\n",
    "            with tf.variable_scope('conv2', reuse=True):\n",
    "                self.WK2 = tf.get_variable('weights')\n",
    "                \n",
    "            #Layer: Conv3\n",
    "            self.conv3 = layers.convolution2d(\n",
    "                inputs=self.conv2,\n",
    "                data_format='NCHW',\n",
    "                num_outputs=num_kern[2],\n",
    "                kernel_size=s[1],\n",
    "                stride=1,\n",
    "                padding='VALID',\n",
    "                activation_fn=tf.nn.relu,#None\n",
    "                normalizer_fn=layers.batch_norm,\n",
    "                normalizer_params=bn_params,\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=init_scaleK),\n",
    "                scope='conv3')\n",
    "            with tf.variable_scope('conv3', reuse=True):\n",
    "                self.WK3 = tf.get_variable('weights')\n",
    "            \n",
    "            #batch_norm op\n",
    "            self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            \n",
    "            #Location layer - each neuron has one location mask\n",
    "            self.WM = tf.Variable(self.WM_init,dtype=tf.float32, name='WM',trainable=False)\n",
    "            #self.WMx = tf.Variable(self.WM_initx,dtype=tf.float32, name='WMx')\n",
    "            #self.WMy = tf.Variable(self.WM_inity,dtype=tf.float32, name='WMy')\n",
    "            #self.WMa = tf.abs(self.WM)\n",
    "            #self.WM = tf.transpose(tf.reshape(tf.einsum('ki,kj->kij', self.WMx, self.WMy),[N,s[2]**2]))\n",
    "            self.mask = tf.reshape(tf.matmul(tf.reshape(self.conv3,[-1,s[2]**2]),\n",
    "                                   self.WM),[-1,num_kern[-1],N])\n",
    "            \n",
    "            #Weighing Layer\n",
    "            self.WW = tf.Variable(self.WW_init,dtype=tf.float32, name='WW')\n",
    "            #self.WWn = self.WW# / tf.sqrt(tf.reduce_sum(tf.square(self.WW),0,keep_dims=True))\n",
    "            self.WWn = tf.abs(self.WW)\n",
    "            \n",
    "            #Predicted Output\n",
    "            self.Y_ = tf.squeeze(tf.transpose(tf.reduce_sum(tf.multiply(self.mask,\n",
    "                            self.WWn), 1, keep_dims=True)))#N x D\n",
    "            \n",
    "            #Regularization\n",
    "            self.regM = tf.reduce_sum(tf.abs(self.WM)) #L1 Loss on mask\n",
    "            self.regW = tf.reduce_sum(tf.abs(self.WW)) #L1 Loss on read out weights\n",
    "            \n",
    "            #Define a loss function\n",
    "            self.res = self.Y_-self.Y\n",
    "            self.MSE = tf.reduce_sum(tf.reduce_mean(self.res * self.res,1))\n",
    "            #self.poisson = tf.reduce_sum(tf.nn.log_poisson_loss(tf.log(self.Y_),self.Y))\n",
    "            self.loss = self.MSE + rM*self.regM + rW*self.regW\n",
    "            \n",
    "            #Define a training graph\n",
    "            self.step_size= tf.placeholder(tf.float32)\n",
    "            self.training = tf.train.AdamOptimizer(self.step_size).minimize(self.loss)\n",
    "            #self.training_noM = tf.train.AdamOptimizer(self.step_size).minimize(self.loss,\n",
    "            #                                   var_list=[self.WK,self.WW])\n",
    "            \n",
    "            # Create a saver.\n",
    "            self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:58:08.487184",
     "start_time": "2017-09-08T10:58:07.492395"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! Training CNN-NL\n",
    "def train(init_scaleK,init_scaleM,init_scaleW,init_lr,num_kern,tmp_sta,max_runs,rM,rW,s,N,\n",
    "          X_train,X_val,X_test,Y_train,Y_val,GT_test,batch_size,verbose=False):\n",
    "    \n",
    "    #Storing:\n",
    "    MSE_train = [] # MSE on train set, reps x runs/100\n",
    "    tmp_MT = []#dummy for storing the above during repetition for Xruns\n",
    "    MSE_val = []#MSE on validation set, reps x runs/100\n",
    "    tmp_MV = []#dummy for storing the above during repetition for Xruns\n",
    "    MSE_test = []#MSE on test set, reps x 1\n",
    "    WK1 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK1 = []#dummy during run\n",
    "    WK2 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK2 = []#dummy during run\n",
    "    WK3 = []# Kernel - store best weights, reps x s2 x s2\n",
    "    tmp_WK3 = []#dummy during run\n",
    "    WW = []# Read Out Weights - store best weights, reps x s x s\n",
    "    tmp_WW = []#dummy during run\n",
    "    WM = []# Mask\n",
    "    tmp_WM = []\n",
    "    FEV = []#fraction of explained variance, reps x 1\n",
    "\n",
    "    #calculate test variance\n",
    "    gt_test_var = np.sum(np.var(GT_test,axis =1))#explainable output variance\n",
    "    \n",
    "    #initialize current attributes\n",
    "    lr=init_lr\n",
    "    # flags for early stopping\n",
    "    stop_flag = 0\n",
    "    sstop=0\n",
    "\n",
    "    #Init Mask weights\n",
    "    #STA - maximum pixel, use the smoothed STA, better estimation, see above!\n",
    "    tmp = np.random.normal(0,init_scaleM,tmp_sta.shape)\n",
    "    tmp[np.arange(N),np.argmax(abs(tmp_sta),1)] = np.ones(N)\n",
    "    init_mask = tmp.astype(np.float32).T\n",
    "    \n",
    "    #factorization\n",
    "    #tmpx=np.random.normal(0,init_scaleM,[N,s[2]])\n",
    "    #tmpy=np.random.normal(0,init_scaleM,[N,s[2]])\n",
    "    #dumx,dumy=np.unravel_index(np.argmax(abs(tmp_sta),1),(s[2],s[2]))\n",
    "    #tmpx[np.arange(N),dumx]=np.ones(N)\n",
    "    #init_maskx = tmpx.astype(np.float32)\n",
    "    #tmpy[np.arange(N),dumy]=np.ones(N)\n",
    "    #init_masky = tmpy.astype(np.float32)\n",
    "    #print(loc_x[:10],dumx[:10],loc_y[:10],dumy[:10])\n",
    "    \n",
    "    #init Weights\n",
    "    init_weights = (np.ones([num_kern[-1],N])*init_scaleW[0]).astype(np.float32)\n",
    "    if init_scaleW[1]>0:\n",
    "        init_weights = np.random.normal(init_scaleW[0],init_scaleW[1],[num_kern[-1],N])\n",
    "    \n",
    "    #Init model class\n",
    "    model = ModelGraph(s,rM,rW,init_scaleK,init_scaleM,init_scaleW,\n",
    "                       N,num_kern,init_mask,init_weights=init_weights)#,init_kernel)\n",
    "\n",
    "    #validation feed can be outside loop:\n",
    "    #feed_val = {model.X:X_val,\n",
    "    #            model.Y:Y_val,model.istrain:False}\n",
    "    #feed_test = {model.X:X_test,model.Y:GT_test,model.istrain:False}\n",
    "    \n",
    "    #Split up because too large for GPU memory...\n",
    "    num_test=np.int(GT_test.shape[1]/batch_size)\n",
    "    feed_test=[]\n",
    "    for i in range(num_test):\n",
    "        feed_test.append( {model.X:X_test[i*batch_size:(i+1)*batch_size,:,:,:],\n",
    "                        model.Y:GT_test[:,i*batch_size:(i+1)*batch_size],model.istrain:False})\n",
    "    num_val=np.int(Y_val.shape[1]/batch_size)\n",
    "    feed_val=[]\n",
    "    for i in range(num_val):\n",
    "        feed_val.append( {model.X:X_val[i*batch_size:(i+1)*batch_size,:,:,:],\n",
    "                        model.Y:Y_val[:,i*batch_size:(i+1)*batch_size],model.istrain:False})\n",
    "    \n",
    "    ##Start a tf session\n",
    "    with model.graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            #plot initial weights\n",
    "            if verbose:\n",
    "                print('Before Training:')\n",
    "                fig, ax = plt.subplots(1, 4, figsize=[18, 3])\n",
    "                #tmp_wk=model.WK1.eval().reshape([s[1],s[1],3,\n",
    "                #     num_kern[0]])[:,:,0,:].reshape([s2**2,num_kern[0]]).T\n",
    "                b=-1\n",
    "                tmp_ww=model.WW.eval()\n",
    "                tmp_wm=model.WM.eval()[:,0].T#first neuron as example\n",
    "                #for b in range(num_kern[0]):\n",
    "                #    ax[b].imshow(tmp_wk[b,:].reshape([s[1],s[1]]),cmap='bwr',\n",
    "                #                vmin=-max(abs(tmp_wk[b,:])),vmax=max(abs(tmp_wk[b,:])))\n",
    "                #    ax[b].get_xaxis().set_visible(False)\n",
    "                #    ax[b].get_yaxis().set_visible(False)\n",
    "                #    ax[b].set_title('Kernel')\n",
    "                ax[1+b].plot(tmp_ww[:,0])\n",
    "                ax[1+b].set_title('Weights N_1')\n",
    "                ax[2+b].imshow(tmp_wm.reshape([s[2],s[2]]),cmap='bwr',\n",
    "                    vmin=-max(abs(tmp_wm.T)),vmax=max(abs(tmp_wm.T)))\n",
    "                ax[2+b].get_xaxis().set_visible(False)\n",
    "                ax[2+b].get_yaxis().set_visible(False)\n",
    "                ax[2+b].set_title('Mask N_1')\n",
    "                test=np.zeros(num_test*batch_size)\n",
    "                for i in range(num_test):\n",
    "                    test[i*batch_size:(i+1)*batch_size]=sess.run(model.Y_,feed_test[i])[0]\n",
    "                ax[3+b].plot(GT_test[0], test, '.')\n",
    "                #ax[3+b].plot(GT_test[0], sess.run(model.Y_,feed_test)[0], '.')\n",
    "                xx = [-.4, .4]\n",
    "                ax[3+b].plot(xx, xx)\n",
    "                ax[3+b].axis('equal')\n",
    "                ax[3+b].set_title('true vs predicted')\n",
    "                ax[4+b].plot(tmp_ww)\n",
    "                ax[4+b].set_title('All Weights')\n",
    "                plt.show()\n",
    "                print('Loss: %s mask'%(model.regM.eval()*rM),\n",
    "                      'Loss: %s weights'%(model.regW.eval()*rW))\n",
    "\n",
    "            #Batches - define list of starting-indices for individual batches in data set:\n",
    "            #if there is less training data than batch size\n",
    "            batch_size = np.min([batch_size,X_train.shape[0]])\n",
    "            batch_ind = np.arange(0,X_train.shape[0],batch_size)\n",
    "            #number of selected batch\n",
    "            batch = 0\n",
    "\n",
    "            #Optimization runs\n",
    "            #minimal verbose\n",
    "            #verb_time=0\n",
    "            #from IPython import display\n",
    "            #fig,ax=plt.subplots(1,2, figsize=[10, 5])\n",
    "            for j in range(1,max_runs):\n",
    "\n",
    "                #when there is no further complete batch\n",
    "                if batch==len(batch_ind):\n",
    "                    #shuffle data and start again:\n",
    "                    ind = np.random.permutation(X_train.shape[0])\n",
    "                    X_train = X_train[ind,:,:,:]\n",
    "                    Y_train = Y_train[:,ind]\n",
    "                    batch = 0\n",
    "\n",
    "                #take a batch\n",
    "                X_batch = X_train[batch_ind[batch]:batch_ind[batch]+batch_size,:,:,:]\n",
    "                Y_batch = Y_train[:,batch_ind[batch]:batch_ind[batch]+batch_size]\n",
    "                batch +=1\n",
    "                \n",
    "                #Training feed:\n",
    "                feed_dict ={model.step_size:lr,model.X:X_batch,\n",
    "                            model.Y:Y_batch,model.istrain:True}\n",
    "\n",
    "                # Training with current batch:\n",
    "                sess.run([model.training, model.update_ops],feed_dict)\n",
    "                \n",
    "                #Early Stopping - check if MSE doesn't increase\n",
    "                if j%100==0:\n",
    "\n",
    "                    model.saver.save(sess, 'Batty_bn_checkpoint', global_step=int(j/100))\n",
    "\n",
    "                    #exponentially decreasing learning rate?\n",
    "                    #lr *= .99\n",
    "\n",
    "                    # Store MSE on train:\n",
    "                    \n",
    "                    tmp_MT.append(model.MSE.eval(feed_dict))\n",
    "\n",
    "                    #check MSE on validation set and store the parameters\n",
    "                    #tmp_MV.append(model.MSE.eval(feed_val))\n",
    "                    val=np.zeros(num_val)\n",
    "                    for i in range(num_val):\n",
    "                        val[i]=sess.run(model.MSE,feed_val[i])\n",
    "                    tmp_MV.append(np.mean(val))\n",
    "                    tmp_WK1.append(model.WK1.eval())\n",
    "                    tmp_WK2.append(model.WK2.eval())\n",
    "                    tmp_WK3.append(model.WK3.eval())\n",
    "                    tmp_WW.append(model.WW.eval())\n",
    "                    tmp_WM.append(model.WM.eval())\n",
    "\n",
    "                    #Best run\n",
    "                    if len(tmp_MV)>5:#burn in \n",
    "                        tmp_min_ind = np.argmin(tmp_MV[5:])+5\n",
    "                    else:\n",
    "                        tmp_min_ind = len(tmp_MV)-1\n",
    "                        \n",
    "                    #minimal verbose\n",
    "                    #start=time.time()\n",
    "                    #print('run=%s,lr=%s,MSE-train=%s,MSE-val=%s'%(j,lr,\n",
    "                    #            tmp_MT[-1],tmp_MV[-1]))\n",
    "                    if j>99100:\n",
    "                        #test=np.zeros(num_test)\n",
    "                        #for i in range(num_test):\n",
    "                        #    test[i]=sess.run(model.MSE,feed_test[i])\n",
    "                        #MSE_test = np.mean(test)\n",
    "                        \n",
    "                        ax[0].plot(np.arange(len(tmp_MT)-1),tmp_MT[1:],'r')\n",
    "                        ax[0].plot(np.arange(len(tmp_MT)-1),tmp_MV[1:],'g')\n",
    "                        ax[0].legend(['Train MSE','Val MSE'])\n",
    "                        #ax[0].legend(['Val MSE'])\n",
    "                        ax[0].set_title('run=%s,lr=%s,plot_time=%s,MSE-train=%s,MSE-val=%s'%(j,\n",
    "                                      lr,np.round(verb_time),tmp_MT[-1],tmp_MV[-1]))\n",
    "                        #ax[1].set_title('MSE-val=%s, FEV=%s'%(tmp_MV[-1],\n",
    "                        #        (1 - (MSE_test/gt_test_var))))\n",
    "                        \n",
    "                    #verb_time+=time.time()-start\n",
    "\n",
    "                    ##Analytics - Display progress\n",
    "                    if verbose:\n",
    "                        #to calculate FEV\n",
    "                        test=np.zeros(num_test)\n",
    "                        for i in range(num_test):\n",
    "                            test[i]=sess.run(model.MSE,feed_test[i])\n",
    "                        MSE_gt = np.mean(test)\n",
    "                        print('Runs: %s; MSE - train: %s, val: %s; lr = %s'%\n",
    "                              (j,tmp_MT[-1],tmp_MV[-1],lr))\n",
    "                        print('Loss: %s mask'%(model.regM.eval()*rM),\n",
    "                              'Loss: %s weights'%(model.regW.eval()*rW))\n",
    "                        print('latest: ',tmp_MV[-1])\n",
    "                        print('FEV = ',(1 - (MSE_gt/gt_test_var)))\n",
    "                        #print(len(tmp_MV),tmp_min_ind)\n",
    "                        print('best: ',tmp_min_ind,tmp_MV[tmp_min_ind])\n",
    "\n",
    "                        #Plot learned weight example\n",
    "                        fig, ax = plt.subplots(1, 5, figsize=[18, 3])\n",
    "                        #tmp_wk=model.WK1.eval().reshape([s[1],s[1],3,\n",
    "                        # num_kern[0]])[:,:,0,:].reshape([s[1]**2,num_kern[0]]).T\n",
    "                        tmp_ww=model.WW.eval()\n",
    "                        tmp_wm=model.WM.eval()[:,0].T#first neuron as example\n",
    "                        #for b in range(num_kern[0]):\n",
    "                        #    ax[b].imshow(tmp_wk[b,:].reshape([s[1],s[1]]),cmap='bwr',\n",
    "                        #                vmin=-max(abs(tmp_wk[b,:])),vmax=max(abs(tmp_wk[b,:])))\n",
    "                        #    ax[b].get_xaxis().set_visible(False)\n",
    "                        #    ax[b].get_yaxis().set_visible(False)\n",
    "                        #    ax[b].set_title('Kernel')\n",
    "                        b=-1\n",
    "                        ax[1+b].plot(tmp_ww[:,0])\n",
    "                        ax[1+b].set_title('Weights N_1')\n",
    "                        ax[2+b].imshow(tmp_wm.reshape([s3,s3]),cmap='bwr',\n",
    "                            vmin=-max(abs(tmp_wm.T)),vmax=max(abs(tmp_wm.T)))\n",
    "                        ax[2+b].get_xaxis().set_visible(False)\n",
    "                        ax[2+b].get_yaxis().set_visible(False)\n",
    "                        ax[2+b].set_title('Mask N_1')\n",
    "                        ax[2+b].axhline(y=loc_y[0]+1, xmin=0, xmax=32, linewidth=1, color = 'g')\n",
    "                        ax[2+b].axvline(x=loc_x[0]-1, ymin=0, ymax =32, linewidth=1, color='g')\n",
    "                        test=np.zeros(num_test*batch_size)\n",
    "                        for i in range(num_test):\n",
    "                            test[i*batch_size:(i+1)*batch_size]=sess.run(model.Y_,feed_test[i])[0]\n",
    "                        ax[3+b].plot(GT_test[0], test, '.')\n",
    "                        #ax[3+b].plot(GT_test[0], sess.run(model.Y_,feed_test)[0], '.')\n",
    "                        xx = [-.4, .4]\n",
    "                        ax[3+b].plot(xx, xx)\n",
    "                        ax[3+b].axis('equal')\n",
    "                        ax[3+b].set_title('true vs predicted')\n",
    "                        ax[4+b].plot(tmp_ww[:,:250],'r')\n",
    "                        ax[4+b].plot(tmp_ww[:,250:500],'b')\n",
    "                        ax[4+b].plot(tmp_ww[:,500:750],'y')\n",
    "                        ax[4+b].plot(tmp_ww[:,750:],'g')\n",
    "                        ax[4+b].set_title('All Weights')\n",
    "                        ax[5+b].plot(tmp_MV)\n",
    "                        ax[5+b].plot(tmp_MT)\n",
    "                        ax[5+b].set_ylim([min(tmp_MT),2*np.median(tmp_MV)-min(tmp_MT)])\n",
    "                        ax[5+b].legend(['MSE Val','MSE Train'])\n",
    "                        plt.show()\n",
    "\n",
    "                    ##Early Stopping - if latest validation MSE is not minimum\n",
    "                    if tmp_min_ind != len(tmp_MV)-1:\n",
    "                        stop_flag +=1\n",
    "                        if stop_flag>=8:\n",
    "                            lr *= .1\n",
    "                            #set back to previous best?\n",
    "                            #model.saver.restore(sess, 'bn_checkpoint-%s'%(tmp_min_ind+1))\n",
    "                            #print('back to MSE-val = ',model.MSE_test.eval(feed_val))\n",
    "                            stop_flag = 0\n",
    "                            sstop +=1\n",
    "                            if sstop==3:#lower the lr x times\n",
    "                                break\n",
    "                                \n",
    "                    else:#if latest value is best, reset\n",
    "                        stop_flag = 0\n",
    "\n",
    "            #Best run\n",
    "            tmp_min_ind = np.argmin(tmp_MV)\n",
    "\n",
    "            #Store MSEs\n",
    "            MSE_train = tmp_MT#list\n",
    "            MSE_val = tmp_MV#list\n",
    "            \n",
    "            #Store best weights (i.e. lowest validation MSE)\n",
    "            WK1 = tmp_WK1[tmp_min_ind]#s2 x s2\n",
    "            WK2 = tmp_WK2[tmp_min_ind]#s2 x s2\n",
    "            WK3 = tmp_WK3[tmp_min_ind]#s2 x s2\n",
    "            WW = tmp_WW[tmp_min_ind]# N x s*s\n",
    "            WM = tmp_WM[tmp_min_ind]# num_kern x N\n",
    "\n",
    "            #Assign the best weights to model graph \n",
    "            model.saver.restore(sess, './Batty_bn_checkpoint-%s'%(tmp_min_ind+1))\n",
    "            \n",
    "            #clean checkpoints\n",
    "            files = os.listdir()\n",
    "            for file in files:\n",
    "                if file.startswith(\"Batty_bn_checkpoint\"):\n",
    "                    os.remove(file)\n",
    "                    \n",
    "            # Test MSE prediction\n",
    "            #MSE_test = sess.run(model.MSE,feed_test)#1 x 1\n",
    "            test=np.zeros(num_test)\n",
    "            #test_L=np.zeros(num_test)\n",
    "            for i in range(num_test):\n",
    "                test[i]=sess.run(model.MSE,feed_test[i])\n",
    "            #    test_L[i]=sess.run(model.poisson,feed_test[i])\n",
    "            MSE_test = np.mean(test)\n",
    "            #Loss_test = np.mean(Loss)\n",
    "            \n",
    "            #FEV - fraction of explainable variance\n",
    "            FEV = 1 - (MSE_test/gt_test_var)#1 x 1\n",
    "\n",
    "            #Predicted outputs\n",
    "            #Y_ = sess.run(model.Y_,feed_test)\n",
    "            test=np.zeros([N,num_test*batch_size])\n",
    "            for i in range(num_test):\n",
    "                test[:,i*batch_size:(i+1)*batch_size]=sess.run(model.Y_,feed_test[i])\n",
    "            Y_ = test\n",
    "            \n",
    "            #FEV per cell\n",
    "            fev_cell=1-(np.mean((Y_-GT_test)**2,1)/np.var(GT_test,1))\n",
    "\n",
    "            #Output\n",
    "            log=('Stop at run %s; MSE on validation set: %s'% (j,MSE_val[tmp_min_ind]),\n",
    "                  'MSE on test set: %s; FEV: %s' % (MSE_test, FEV))\n",
    "            print(log)\n",
    "\n",
    "    return (WK1,WK2,WK3,WM,WW,MSE_train,MSE_val,MSE_test,FEV,fev_cell,Y_,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4,8,16 Types (64 each) - D=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### grid search\n",
    "reps = 5\n",
    "\n",
    "Neurons = [4*64,8*64,16*64]\n",
    "\n",
    "start=time.time()\n",
    "#Seeds\n",
    "np_seed=1234\n",
    "np.random.seed(np_seed)\n",
    "tf_seed=1234\n",
    "\n",
    "#Data, keep test set for later\n",
    "N=Neurons[-1]\n",
    "num_test = 2**10\n",
    "ind = np.random.choice(X.shape[0],X.shape[0],replace=False)\n",
    "X_TEST = X[ind[:num_test],...]\n",
    "GT_TEST = ((Y[:,ind[:num_test]].T / np.mean(Y,1)).T * .1)[:N,:]#mean response=.1\n",
    "X_try = X[ind[num_test:],...]\n",
    "Y_try = Y[:N,ind[num_test:]]\n",
    "\n",
    "#Data\n",
    "D=2**12\n",
    "split_data=True#set true for large nets to fit on GPU memory\n",
    "batch_size = 64\n",
    "D=min(D,Y_try.shape[1])#number of training+val images\n",
    "num_val = np.min([D//8,num_test])#limit validation set to test set size\n",
    "num_train = D-num_val\n",
    "\n",
    "Y_train,Y_val,X_train,X_val,X_test,GT_test = splitting_data(X=X_try,Y=Y_try,\n",
    "   num_train=num_train,num_test=num_test,num_val=num_val,noise=True,mean_response=.1)\n",
    "\n",
    "print('Images for Training: %s, Validation: %s, Testing %s'%(\n",
    "      Y_train.shape[1],Y_val.shape[1],GT_test.shape[1]))\n",
    "\n",
    "#Ground truth locations\n",
    "GT_mask = np.hstack([loc_y.reshape([-1,1]),loc_x.reshape([-1,1])])#[ind,:]\n",
    "\n",
    "#Parameters:\n",
    "s1=44#width=heigth of image\n",
    "s2=5\n",
    "s3=32\n",
    "n_B = [32,64,0]#number of bases to learn\n",
    "init_scaleK = .01#Kernel\n",
    "init_scaleM = .001#Mask\n",
    "init_scaleW = [0,.01]#Read out Weights,    1/n_B[-1]\n",
    "lr= .001 # initial learning rate\n",
    "rM = 0#.05#regularization on Mask\n",
    "rW = [.00001,.0001,.001,.01,.1]#tried:.005\n",
    "#loop parameters\n",
    "max_runs = 150000 # training steps\n",
    "batch_size = 64\n",
    "    \n",
    "#Clean previous checkpoints\n",
    "files = os.listdir()\n",
    "for file in files:\n",
    "    if file.startswith(\"Batty_bn_checkpoint\"):\n",
    "        os.remove(file)\n",
    "\n",
    "#calculate spike triggered average to initialize location masks\n",
    "X_sta=np.mean(X_train,axis=1).reshape([-1,s1**2]).T\n",
    "sta,_=STA(X_sta,Y_train,s3,s3//2)\n",
    "\n",
    "\n",
    "print('it took %s s to preprocess data'%(time.time()-start))\n",
    "\n",
    "Val_c4 = np.zeros([reps,5])#rep,reg (mean over neurons)\n",
    "Fev_c4 = np.zeros([reps,5,Neurons[0]])\n",
    "Val_c8 = np.zeros([reps,5])#rep,reg (mean over neurons)\n",
    "Fev_c8 = np.zeros([reps,5,Neurons[1]])\n",
    "Val_c16 = np.zeros([reps,5])#rep,reg (mean over neurons)\n",
    "Fev_c16 = np.zeros([reps,5,Neurons[2]])\n",
    "\n",
    "for rep in range(reps):\n",
    "    with open(\"batty_log.txt\", \"a\") as file:\n",
    "        print('repetition ',rep, file=file)\n",
    "    for n in range(len(Neurons)):\n",
    "        n_B[2] = Neurons[n]//64\n",
    "        init_scaleW[0] = 1/n_B[2]\n",
    "        with open(\"batty_log.txt\", \"a\") as file:\n",
    "            print('neurons %s, kernels/types %s'%(Neurons[n],n_B[2]),file=file)\n",
    "        for reg in range(len(rW)):\n",
    "            with open(\"batty_log.txt\", \"a\") as file:\n",
    "                print('regularization = ',rW[reg],file=file)\n",
    "    \n",
    "            (WK1,WK2,WK3,WM_tmp,WW_tmp,MSE_train,MSE_val,\n",
    "             MSE_test,FEV,fev_c,Y_tmp,log) = train(init_scaleK,\n",
    "                       init_scaleM,init_scaleW,lr,n_B,sta[:Neurons[n],:],\n",
    "                       max_runs,rM,rW[reg],[s1,s2,s3],Neurons[n],X_train,X_val,X_TEST,\n",
    "                       Y_train[:Neurons[n],:],Y_val[:Neurons[n],:],\n",
    "                       GT_TEST[:Neurons[n],:],batch_size,verbose=False)\n",
    "            \n",
    "            if n==0:\n",
    "                Val_c4[rep,reg] = np.min(MSE_val)\n",
    "                Fev_c4[rep,reg,:] = fev_c\n",
    "\n",
    "            if n==1:\n",
    "                Val_c8[rep,reg] = np.min(MSE_val)\n",
    "                Fev_c8[rep,reg,:] = fev_c\n",
    "\n",
    "            if n==2:\n",
    "                Val_c16[rep,reg] = np.min(MSE_val)\n",
    "                Fev_c16[rep,reg,:] = fev_c\n",
    "\n",
    "            #saving text output to log file\n",
    "            with open(\"batty_log.txt\", \"a\") as file:\n",
    "                print(log, file=file)\n",
    "            np.savez('batty_4-8-16_types',\n",
    "                     Val_c4=Val_c4,\n",
    "                     Fev_c4=Fev_c4,\n",
    "                     Val_c8=Val_c8,\n",
    "                     Fev_c8=Fev_c8,\n",
    "                     Val_c16=Val_c16,\n",
    "                     Fev_c16=Fev_c16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
